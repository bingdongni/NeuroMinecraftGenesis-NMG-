#!/usr/bin/env python3
"""
é›†æˆæµ‹è¯•ç³»ç»Ÿä½¿ç”¨ç¤ºä¾‹
Integrated Testing System Usage Example

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨é›†æˆæµ‹è¯•ç³»ç»Ÿçš„å„ç§åŠŸèƒ½
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from utils.testing.integrated_testing import IntegratedTestingSystem, TestResult, PerformanceMetrics

def demo_basic_usage():
    """æ¼”ç¤ºåŸºæœ¬ä½¿ç”¨"""
    print("ğŸš€ NeuroMinecraftGenesis é›†æˆæµ‹è¯•ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 50)
    
    # 1. åˆ›å»ºæµ‹è¯•ç³»ç»Ÿå®ä¾‹
    print("1. åˆ›å»ºæµ‹è¯•ç³»ç»Ÿå®ä¾‹...")
    testing_system = IntegratedTestingSystem("config/integrated_testing_config.json")
    print("âœ… æµ‹è¯•ç³»ç»Ÿåˆ›å»ºæˆåŠŸ")
    
    # 2. è¿è¡ŒåŠŸèƒ½æµ‹è¯•
    print("\n2. è¿è¡ŒåŠŸèƒ½æµ‹è¯•...")
    functional_results = testing_system.run_functional_tests()
    print(f"âœ… åŠŸèƒ½æµ‹è¯•å®Œæˆï¼Œå…± {len(functional_results)} ä¸ªæµ‹è¯•")
    
    # 3. è¿è¡Œæ€§èƒ½æµ‹è¯•
    print("\n3. è¿è¡Œæ€§èƒ½æµ‹è¯•...")
    performance_results = testing_system.run_performance_tests()
    print(f"âœ… æ€§èƒ½æµ‹è¯•å®Œæˆï¼Œå…± {len(performance_results)} ä¸ªåŸºå‡†æµ‹è¯•")
    
    # 4. è¿è¡Œå…¼å®¹æ€§æµ‹è¯•
    print("\n4. è¿è¡Œå…¼å®¹æ€§æµ‹è¯•...")
    compatibility_results = testing_system.run_compatibility_tests()
    print(f"âœ… å…¼å®¹æ€§æµ‹è¯•å®Œæˆï¼Œå…± {len(compatibility_results)} ä¸ªå¹³å°æµ‹è¯•")
    
    # 5. è¿è¡Œç”¨æˆ·ä½“éªŒæµ‹è¯•
    print("\n5. è¿è¡Œç”¨æˆ·ä½“éªŒæµ‹è¯•...")
    ux_results = testing_system.run_ux_tests()
    print(f"âœ… ç”¨æˆ·ä½“éªŒæµ‹è¯•å®Œæˆï¼Œå…± {len(ux_results)} ä¸ªæµ‹è¯•")
    
    # 6. å‡†å¤‡GitHubå‘å¸ƒ
    print("\n6. å‡†å¤‡GitHubå‘å¸ƒ...")
    deployment_data = testing_system.prepare_github_deployment()
    print("âœ… GitHubå‘å¸ƒå‡†å¤‡å®Œæˆ")
    
    # 7. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    print("\n7. ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š...")
    comprehensive_results = testing_system.run_all_tests()
    
    # 8. æ˜¾ç¤ºæ‘˜è¦ç»“æœ
    print("\nğŸ“Š æµ‹è¯•æ‘˜è¦:")
    print("=" * 50)
    
    summary = comprehensive_results.get("summary", {})
    overview = summary.get("test_overview", {})
    
    print(f"åŠŸèƒ½æµ‹è¯•: {overview.get('functional_tests', {}).get('passed', 0)}/{overview.get('functional_tests', {}).get('total', 0)} é€šè¿‡")
    print(f"æ€§èƒ½æµ‹è¯•: {len(performance_results)} é¡¹åŸºå‡†æµ‹è¯•")
    print(f"å…¼å®¹æ€§æµ‹è¯•: {len(compatibility_results)} ä¸ªå¹³å°")
    print(f"ç”¨æˆ·ä½“éªŒæµ‹è¯•: {overview.get('ux_tests', {}).get('passed', 0)}/{overview.get('ux_tests', {}).get('total', 0)} é€šè¿‡")
    
    quality = summary.get("quality_metrics", {})
    deployment = quality.get("deployment_readiness", {})
    
    print(f"éƒ¨ç½²å°±ç»ªæ€§: {deployment.get('status', 'unknown')} (å¾—åˆ†: {deployment.get('score', 0):.2f})")
    
    print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
    
    return comprehensive_results

def demo_individual_modules():
    """æ¼”ç¤ºå•ç‹¬æ¨¡å—çš„ä½¿ç”¨"""
    print("\nğŸ”§ å•ç‹¬æ¨¡å—æ¼”ç¤º")
    print("=" * 50)
    
    testing_system = IntegratedTestingSystem()
    
    # ä»…åŠŸèƒ½æµ‹è¯•
    print("æ‰§è¡ŒåŠŸèƒ½æµ‹è¯•...")
    func_results = testing_system.run_functional_tests()
    print(f"åŠŸèƒ½æµ‹è¯•ç»“æœ: {len(func_results)} ä¸ªæµ‹è¯•")
    
    # ä»…æ€§èƒ½æµ‹è¯•
    print("æ‰§è¡Œæ€§èƒ½æµ‹è¯•...")
    perf_results = testing_system.run_performance_tests()
    print(f"æ€§èƒ½æµ‹è¯•ç»“æœ: {len(perf_results)} ä¸ªåŸºå‡†æµ‹è¯•")
    
    # ä»…å…¼å®¹æ€§æµ‹è¯•
    print("æ‰§è¡Œå…¼å®¹æ€§æµ‹è¯•...")
    comp_results = testing_system.run_compatibility_tests()
    print(f"å…¼å®¹æ€§æµ‹è¯•ç»“æœ: {len(comp_results)} ä¸ªå¹³å°æµ‹è¯•")
    
    # ä»…ç”¨æˆ·ä½“éªŒæµ‹è¯•
    print("æ‰§è¡Œç”¨æˆ·ä½“éªŒæµ‹è¯•...")
    ux_results = testing_system.run_ux_tests()
    print(f"ç”¨æˆ·ä½“éªŒæµ‹è¯•ç»“æœ: {len(ux_results)} ä¸ªæµ‹è¯•")
    
    # ä»…GitHubå‘å¸ƒå‡†å¤‡
    print("å‡†å¤‡GitHubå‘å¸ƒ...")
    deploy_results = testing_system.prepare_github_deployment()
    print("GitHubå‘å¸ƒå‡†å¤‡å®Œæˆ")

def demo_custom_configuration():
    """æ¼”ç¤ºè‡ªå®šä¹‰é…ç½®"""
    print("\nâš™ï¸ è‡ªå®šä¹‰é…ç½®æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºè‡ªå®šä¹‰é…ç½®
    custom_config = {
        "functional_tests": {
            "enabled": True,
            "timeout": 60,
            "critical_modules": ["brain", "evolution"]
        },
        "performance_tests": {
            "enabled": True,
            "benchmark_duration": 30
        }
    }
    
    # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®è¿è¡Œæµ‹è¯•
    testing_system = IntegratedTestingSystem()
    results = testing_system.run_all_tests(config_overrides=custom_config)
    
    print("ä½¿ç”¨è‡ªå®šä¹‰é…ç½®å®Œæˆæµ‹è¯•")
    return results

def demo_test_result_analysis():
    """æ¼”ç¤ºæµ‹è¯•ç»“æœåˆ†æ"""
    print("\nğŸ“ˆ æµ‹è¯•ç»“æœåˆ†ææ¼”ç¤º")
    print("=" * 50)
    
    testing_system = IntegratedTestingSystem()
    results = testing_system.run_all_tests()
    
    # åˆ†æåŠŸèƒ½æµ‹è¯•ç»“æœ
    functional_tests = results.get("functional_tests", [])
    print("åŠŸèƒ½æµ‹è¯•åˆ†æ:")
    for test in functional_tests:
        if test.get("status") == "FAIL":
            print(f"  âŒ å¤±è´¥: {test.get('test_name')} - {test.get('error_message')}")
        elif test.get("status") == "PASS":
            print(f"  âœ… é€šè¿‡: {test.get('test_name')}")
    
    # åˆ†ææ€§èƒ½æµ‹è¯•ç»“æœ
    performance_tests = results.get("performance_tests", [])
    print("\\næ€§èƒ½æµ‹è¯•åˆ†æ:")
    for perf in performance_tests:
        efficiency = perf.get("resource_efficiency", 0)
        status = "ğŸŸ¢" if efficiency >= 80 else "ğŸŸ¡" if efficiency >= 60 else "ğŸ”´"
        print(f"  {status} èµ„æºæ•ˆç‡: {efficiency:.1f}% (CPU: {perf.get('cpu_usage', 0):.1f}%, Memory: {perf.get('memory_usage', 0):.1f}%)")
    
    # åˆ†æéƒ¨ç½²å°±ç»ªæ€§
    summary = results.get("summary", {})
    quality_metrics = summary.get("quality_metrics", {})
    deployment_readiness = quality_metrics.get("deployment_readiness", {})
    
    print("\\néƒ¨ç½²å°±ç»ªæ€§åˆ†æ:")
    print(f"  çŠ¶æ€: {deployment_readiness.get('status', 'unknown')}")
    print(f"  å¾—åˆ†: {deployment_readiness.get('score', 0):.2f}")
    print(f"  æ¶ˆæ¯: {deployment_readiness.get('message', 'æ— ä¿¡æ¯')}")
    
    # è¯†åˆ«å…³é”®é—®é¢˜
    critical_issues = quality_metrics.get("critical_issues", [])
    if critical_issues:
        print("\\nğŸš¨ å…³é”®é—®é¢˜:")
        for issue in critical_issues:
            print(f"  {issue.get('type', 'æœªçŸ¥')} - {issue.get('description', 'æ— æè¿°')}")
    else:
        print("\\nâœ… æœªå‘ç°å…³é”®é—®é¢˜")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="é›†æˆæµ‹è¯•ç³»ç»Ÿä½¿ç”¨ç¤ºä¾‹")
    parser.add_argument("--demo", choices=["basic", "individual", "custom", "analysis"], 
                       default="basic", help="æ¼”ç¤ºç±»å‹")
    parser.add_argument("--config", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    try:
        if args.demo == "basic":
            demo_basic_usage()
        elif args.demo == "individual":
            demo_individual_modules()
        elif args.demo == "custom":
            demo_custom_configuration()
        elif args.demo == "analysis":
            demo_test_result_analysis()
        else:
            print("æœªçŸ¥çš„æ¼”ç¤ºç±»å‹")
            
    except Exception as e:
        print(f"æ¼”ç¤ºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()