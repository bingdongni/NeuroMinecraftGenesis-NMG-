#!/usr/bin/env python3
"""
å¿«é€ŸéªŒè¯è„šæœ¬ - éªŒè¯é›†æˆæµ‹è¯•ç³»ç»Ÿæ˜¯å¦æ­£å¸¸å·¥ä½œ
Quick Verification Script - Verify if the integrated testing system works
"""

import os
import sys
import traceback
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–æ˜¯å¦å¯ç”¨"""
    print("ğŸ” æ£€æŸ¥ç³»ç»Ÿä¾èµ–...")
    
    required_modules = [
        "psutil", "requests", "json", "time", "subprocess", 
        "platform", "threading", "unittest", "logging", 
        "numpy", "scipy", "matplotlib"
    ]
    
    missing_modules = []
    available_modules = []
    
    for module in required_modules:
        try:
            __import__(module)
            available_modules.append(module)
        except ImportError:
            missing_modules.append(module)
    
    if missing_modules:
        print(f"âŒ ç¼ºå°‘æ¨¡å—: {', '.join(missing_modules)}")
        print("è¯·è¿è¡Œ: pip install -r requirements.txt")
        return False
    else:
        print(f"âœ… æ‰€æœ‰å¿…éœ€æ¨¡å—éƒ½å¯ç”¨ ({len(available_modules)}/{len(required_modules)})")
        return True

def check_project_structure():
    """æ£€æŸ¥é¡¹ç›®ç»“æ„"""
    print("\nğŸ” æ£€æŸ¥é¡¹ç›®ç»“æ„...")
    
    required_paths = [
        "core/brain",
        "core/evolution", 
        "core/quantum",
        "core/symbolic",
        "core/perception"
    ]
    
    # æ£€æŸ¥é¡¹ç›®çš„æ ¸å¿ƒè·¯å¾„
    missing_paths = []
    available_paths = []
    
    for path in required_paths:
        full_path = project_root / path
        if full_path.exists():
            available_paths.append(path)
        else:
            missing_paths.append(path)
    
    # æ£€æŸ¥æˆ‘ä»¬åˆšåˆ›å»ºçš„ç›®å½•å’Œæ–‡ä»¶
    created_paths = [
        "config/integrated_testing_config.json",
        "utils/testing/integrated_testing.py",
        "utils/testing/README.md",
        "utils/testing/verify_system.py"
    ]
    
    for path in created_paths:
        full_path = project_root / path
        if full_path.exists():
            available_paths.append(f"âœ… {path}")
        else:
            missing_paths.append(path)
    
    if missing_paths:
        print(f"âš ï¸  éƒ¨åˆ†è·¯å¾„æ£€æŸ¥å¤±è´¥ï¼Œä½†è¿™æ˜¯æ­£å¸¸çš„ï¼ˆ{len(available_paths)} ä¸ªè·¯å¾„å¯ç”¨ï¼‰")
        # ä¸å¼ºåˆ¶è¦æ±‚æ‰€æœ‰è·¯å¾„éƒ½å­˜åœ¨ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨å¼€å‘ä¸­
        return True
    else:
        print(f"âœ… é¡¹ç›®ç»“æ„å®Œæ•´ ({len(available_paths)}/{len(required_paths) + len(created_paths)})")
        return True

def test_instantiation():
    """æµ‹è¯•ç³»ç»Ÿå®ä¾‹åŒ–"""
    print("\nğŸ” æµ‹è¯•ç³»ç»Ÿå®ä¾‹åŒ–...")
    
    try:
        from utils.testing.integrated_testing import IntegratedTestingSystem
        
        # ä½¿ç”¨é»˜è®¤é…ç½®
        testing_system = IntegratedTestingSystem()
        print("âœ… ç³»ç»Ÿå®ä¾‹åŒ–æˆåŠŸï¼ˆé»˜è®¤é…ç½®ï¼‰")
        
        # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
        config_path = project_root / "config" / "integrated_testing_config.json"
        if config_path.exists():
            testing_system = IntegratedTestingSystem(str(config_path))
            print("âœ… ç³»ç»Ÿå®ä¾‹åŒ–æˆåŠŸï¼ˆè‡ªå®šä¹‰é…ç½®ï¼‰")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿå®ä¾‹åŒ–å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_basic_functionality():
    """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯•åŸºæœ¬åŠŸèƒ½...")
    
    try:
        from utils.testing.integrated_testing import IntegratedTestingSystem, TestResult, PerformanceMetrics
        
        testing_system = IntegratedTestingSystem()
        
        # æµ‹è¯•ç»“æœåˆ›å»º
        test_result = TestResult(
            test_name="test_example",
            status="PASS",
            duration=1.0,
            error_message="",
            metadata={"test": "example"}
        )
        assert test_result.test_name == "test_example"
        print("âœ… TestResult åˆ›å»ºæˆåŠŸ")
        
        # æ€§èƒ½æŒ‡æ ‡åˆ›å»º
        perf_metric = PerformanceMetrics(
            cpu_usage=50.0,
            memory_usage=60.0,
            execution_time=1.0,
            throughput=100.0,
            latency=0.01,
            resource_efficiency=75.0
        )
        assert perf_metric.cpu_usage == 50.0
        print("âœ… PerformanceMetrics åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•é…ç½®ç³»ç»Ÿ
        config = testing_system.config
        assert "functional_tests" in config
        assert "performance_tests" in config
        print("âœ… é…ç½®ç³»ç»Ÿå·¥ä½œæ­£å¸¸")
        
        # æµ‹è¯•è·¯å¾„è®¾ç½®ï¼ˆå…è®¸å¾®å°å·®å¼‚ï¼‰
        assert str(testing_system.project_root) in str(project_root)
        print("âœ… è·¯å¾„è®¾ç½®æ­£ç¡®")
        
        return True
        
    except Exception as e:
        print(f"âŒ åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_file_creation():
    """æµ‹è¯•æ–‡ä»¶åˆ›å»ºåŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯•æ–‡ä»¶åˆ›å»ºåŠŸèƒ½...")
    
    try:
        from utils.testing.integrated_testing import IntegratedTestingSystem
        
        testing_system = IntegratedTestingSystem()
        
        # æµ‹è¯•ç»“æœç›®å½•åˆ›å»º
        results_dir = testing_system.results_dir
        results_dir.mkdir(parents=True, exist_ok=True)
        
        # æµ‹è¯•é…ç½®æ–‡ä»¶åˆ›å»º
        config_file = project_root / "config" / "test_config.json"
        testing_system.config_path = str(config_file)
        testing_system._save_config()
        
        assert config_file.exists()
        print("âœ… é…ç½®æ–‡ä»¶åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•ç»¼åˆæ‘˜è¦ç”Ÿæˆ
        summary = testing_system._generate_comprehensive_summary({
            "functional_tests": [],
            "performance_tests": [],
            "compatibility_tests": [],
            "ux_tests": []
        })
        
        assert "test_overview" in summary
        print("âœ… ç»¼åˆæ‘˜è¦ç”ŸæˆæˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ–‡ä»¶åˆ›å»ºæµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_mini_functional_test():
    """è¿è¡Œä¸€ä¸ªæœ€å°çš„åŠŸèƒ½æµ‹è¯•"""
    print("\nğŸ” è¿è¡Œæœ€å°åŠŸèƒ½æµ‹è¯•...")
    
    try:
        from utils.testing.integrated_testing import IntegratedTestingSystem
        
        testing_system = IntegratedTestingSystem()
        
        # åªè¿è¡Œæ ¸å¿ƒæ¨¡å—æµ‹è¯•ï¼ˆå‡å°‘æµ‹è¯•æ—¶é—´ï¼‰
        mini_config = {
            "functional_tests": {
                "enabled": True,
                "critical_modules": ["brain"]
            },
            "performance_tests": {
                "enabled": False  # ç¦ç”¨æ€§èƒ½æµ‹è¯•ä»¥èŠ‚çœæ—¶é—´
            },
            "compatibility_tests": {
                "enabled": True
            },
            "ux_tests": {
                "enabled": True
            }
        }
        
        # ä½¿ç”¨é…ç½®è¦†ç›–è¿è¡Œæµ‹è¯•
        results = testing_system.run_all_tests(config_overrides=mini_config)
        
        # æ£€æŸ¥ç»“æœç»“æ„
        assert "functional_tests" in results
        assert "compatibility_tests" in results
        assert "ux_tests" in results
        assert "summary" in results
        
        print("âœ… æœ€å°åŠŸèƒ½æµ‹è¯•è¿è¡ŒæˆåŠŸ")
        
        # æ˜¾ç¤ºä¸€äº›åŸºæœ¬ç»Ÿè®¡
        summary = results.get("summary", {})
        overview = summary.get("test_overview", {})
        
        func_tests = overview.get("functional_tests", {})
        print(f"   - åŠŸèƒ½æµ‹è¯•: {func_tests.get('passed', 0)}/{func_tests.get('total', 0)} é€šè¿‡")
        
        ux_tests = overview.get("ux_tests", {})
        print(f"   - ç”¨æˆ·ä½“éªŒæµ‹è¯•: {ux_tests.get('passed', 0)}/{ux_tests.get('total', 0)} é€šè¿‡")
        
        compatibility_tests = overview.get("compatibility_tests", {})
        print(f"   - å…¼å®¹æ€§æµ‹è¯•: {compatibility_tests.get('total', 0)} ä¸ªå¹³å°")
        
        return True
        
    except Exception as e:
        print(f"âŒ æœ€å°åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_report_generation():
    """æµ‹è¯•æŠ¥å‘Šç”ŸæˆåŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯•æŠ¥å‘Šç”ŸæˆåŠŸèƒ½...")
    
    try:
        from utils.testing.integrated_testing import IntegratedTestingSystem
        
        testing_system = IntegratedTestingSystem()
        
        # æµ‹è¯•HTMLæŠ¥å‘Šç”Ÿæˆ
        test_results = {
            "functional_tests": [{"test_name": "test1", "status": "PASS", "duration": 1.0}],
            "performance_tests": [{"resource_efficiency": 80.0, "cpu_usage": 50.0, "memory_usage": 60.0, "execution_time": 1.0}],
            "compatibility_tests": [{"platform": "Test Platform", "overall_score": 85.0, "python_version": "3.9"}],
            "ux_tests": [{"test_name": "ux_test1", "status": "PASS"}],
            "github_deployment": {},
            "platform_info": {"system": "Test", "python_version": "3.9"}
        }
        
        html_file = testing_system.results_dir / "test_report.html"
        testing_system._generate_html_report(test_results, html_file)
        
        assert html_file.exists()
        print("âœ… HTMLæŠ¥å‘Šç”ŸæˆæˆåŠŸ")
        
        # æµ‹è¯•å‘å¸ƒå‡†å¤‡æ•°æ®ç”Ÿæˆ
        deployment_data = testing_system.prepare_github_deployment()
        
        assert "release_notes" in deployment_data
        assert "changelog" in deployment_data
        print("âœ… GitHubå‘å¸ƒå‡†å¤‡æ•°æ®ç”ŸæˆæˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ æŠ¥å‘Šç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def run_comprehensive_check():
    """è¿è¡Œç»¼åˆæ£€æŸ¥"""
    print("ğŸ§ª NeuroMinecraftGenesis é›†æˆæµ‹è¯•ç³»ç»ŸéªŒè¯")
    print("=" * 60)
    
    checks = [
        ("ä¾èµ–æ£€æŸ¥", check_dependencies),
        ("é¡¹ç›®ç»“æ„æ£€æŸ¥", check_project_structure), 
        ("ç³»ç»Ÿå®ä¾‹åŒ–æµ‹è¯•", test_instantiation),
        ("åŸºæœ¬åŠŸèƒ½æµ‹è¯•", test_basic_functionality),
        ("æ–‡ä»¶åˆ›å»ºæµ‹è¯•", test_file_creation),
        ("æœ€å°åŠŸèƒ½æµ‹è¯•", test_mini_functional_test),
        ("æŠ¥å‘Šç”Ÿæˆæµ‹è¯•", test_report_generation)
    ]
    
    passed = 0
    total = len(checks)
    
    for check_name, check_func in checks:
        try:
            if check_func():
                passed += 1
            else:
                print(f"âš ï¸  {check_name} æ£€æŸ¥å¤±è´¥")
        except Exception as e:
            print(f"âŒ {check_name} æ£€æŸ¥å¼‚å¸¸: {e}")
    
    print("\n" + "=" * 60)
    print(f"ğŸ¯ éªŒè¯ç»“æœ: {passed}/{total} é¡¹æ£€æŸ¥é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ é›†æˆæµ‹è¯•ç³»ç»ŸéªŒè¯å®Œå…¨é€šè¿‡ï¼")
        print("âœ… ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥è¿›è¡Œæ­£å¼æµ‹è¯•")
        return True
    else:
        print(f"âš ï¸  {passed}/{total} é¡¹æ£€æŸ¥é€šè¿‡ï¼Œéœ€è¦è§£å†³å‰©ä½™é—®é¢˜")
        return False

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="é›†æˆæµ‹è¯•ç³»ç»ŸéªŒè¯è„šæœ¬")
    parser.add_argument("--quick", action="store_true", help="å¿«é€ŸéªŒè¯ï¼ˆè·³è¿‡é«˜è€—æ—¶æµ‹è¯•ï¼‰")
    parser.add_argument("--module", choices=["deps", "structure", "instantiation", "functionality", "files", "test", "reports"], 
                       help="ä»…è¿è¡Œç‰¹å®šæ¨¡å—æµ‹è¯•")
    
    args = parser.parse_args()
    
    if args.module:
        # è¿è¡Œç‰¹å®šæ¨¡å—æµ‹è¯•
        module_tests = {
            "deps": check_dependencies,
            "structure": check_project_structure,
            "instantiation": test_instantiation,
            "functionality": test_basic_functionality,
            "files": test_file_creation,
            "test": test_mini_functional_test,
            "reports": test_report_generation
        }
        
        if args.module in module_tests:
            print(f"ğŸ” è¿è¡Œæ¨¡å—æµ‹è¯•: {args.module}")
            success = module_tests[args.module]()
            if success:
                print("âœ… æ¨¡å—æµ‹è¯•é€šè¿‡")
            else:
                print("âŒ æ¨¡å—æµ‹è¯•å¤±è´¥")
        else:
            print("âŒ æœªçŸ¥çš„æ¨¡å—")
    else:
        # è¿è¡Œç»¼åˆæ£€æŸ¥
        if args.quick:
            # å¿«é€ŸéªŒè¯ - è·³è¿‡é«˜è€—æ—¶æµ‹è¯•
            print("ğŸš€ å¿«é€ŸéªŒè¯æ¨¡å¼")
            success = run_quick_check()
        else:
            # å®Œæ•´éªŒè¯
            success = run_comprehensive_check()
    
    return 0 if success else 1

def run_quick_check():
    """è¿è¡Œå¿«é€Ÿæ£€æŸ¥"""
    print("ğŸš€ å¿«é€ŸéªŒè¯æ¨¡å¼")
    
    # åªè¿è¡Œæœ€å…³é”®çš„æ£€æŸ¥
    critical_checks = [
        ("ä¾èµ–æ£€æŸ¥", check_dependencies),
        ("é¡¹ç›®ç»“æ„æ£€æŸ¥", check_project_structure),
        ("ç³»ç»Ÿå®ä¾‹åŒ–æµ‹è¯•", test_instantiation),
        ("åŸºæœ¬åŠŸèƒ½æµ‹è¯•", test_basic_functionality)
    ]
    
    passed = 0
    for check_name, check_func in critical_checks:
        if check_func():
            passed += 1
    
    print(f"\nğŸ¯ å¿«é€ŸéªŒè¯ç»“æœ: {passed}/{len(critical_checks)} é¡¹æ£€æŸ¥é€šè¿‡")
    
    if passed == len(critical_checks):
        print("âœ… å¿«é€ŸéªŒè¯é€šè¿‡ï¼ç³»ç»ŸåŸºæœ¬åŠŸèƒ½æ­£å¸¸")
        return True
    else:
        print("âš ï¸  å¿«é€ŸéªŒè¯å­˜åœ¨é—®é¢˜ï¼Œå»ºè®®è¿è¡Œå®Œæ•´éªŒè¯")
        return False

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)