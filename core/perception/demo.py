#!/usr/bin/env python3
"""
å¤šæ¨¡æ€æ„ŸçŸ¥ç³»ç»Ÿæ¼”ç¤ºè„šæœ¬

è¯¥è„šæœ¬å±•ç¤ºå¦‚ä½•ä½¿ç”¨å¤šæ¨¡æ€æ„ŸçŸ¥ç³»ç»Ÿçš„æ ¸å¿ƒåŠŸèƒ½ï¼Œ
åŒ…æ‹¬å•æ¨¡æ€æµ‹è¯•å’Œå¤šæ¨¡æ€èåˆæ¼”ç¤ºã€‚

ä½œè€…: NeuroMinecraftGenesis
åˆ›å»ºæ—¶é—´: 2025-11-13
"""

import sys
import os
import time
import json
import numpy as np
import threading
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

try:
    from core.perception.multimodal_sensing import (
        MultimodalSensingSystem,
        CameraPerception,
        AudioPerception,
        SpatialPerception,
        WorldModel,
        MultimodalFusion
    )
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·æ£€æŸ¥æ¨¡å—è·¯å¾„å’Œä¾èµ–å®‰è£…")
    sys.exit(1)


class MultimodalDemo:
    """å¤šæ¨¡æ€æ„ŸçŸ¥æ¼”ç¤º"""
    
    def __init__(self):
        self.config = {
            'camera_id': 0,
            'enable_object_detection': True,
            'audio_sample_rate': 16000,
            'num_points': 5000,
            'feature_dim': 256
        }
        self.system = None
        self.is_demo_running = False
    
    def print_header(self, title: str):
        """æ‰“å°æ ‡é¢˜"""
        print("\n" + "=" * 60)
        print(f" {title} ".center(60, "="))
        print("=" * 60)
    
    def print_data(self, title: str, data: Dict[str, Any], indent: int = 0):
        """æ ¼å¼åŒ–æ‰“å°æ•°æ®"""
        prefix = "  " * indent
        print(f"{prefix}{title}:")
        
        for key, value in data.items():
            if isinstance(value, dict):
                print(f"{prefix}  {key}:")
                self.print_data("", value, indent + 2)
            elif isinstance(value, list) and len(value) > 3:
                print(f"{prefix}  {key}: [{len(value)} items]")
            else:
                print(f"{prefix}  {key}: {value}")
    
    def demo_single_modality(self):
        """æ¼”ç¤ºå•æ¨¡æ€åŠŸèƒ½"""
        self.print_header("å•æ¨¡æ€æ„ŸçŸ¥æ¼”ç¤º")
        
        # 1. è§†è§‰æ„ŸçŸ¥æ¼”ç¤º
        print("\n1. è§†è§‰æ„ŸçŸ¥æ¨¡å—æ¼”ç¤º")
        try:
            camera = CameraPerception(enable_object_detection=False)
            camera.start_capture()
            
            print("æ‘„åƒå¤´å¯åŠ¨ä¸­...")
            time.sleep(2)  # ç­‰å¾…æ‘„åƒå¤´ç¨³å®š
            
            frame_data = camera.get_latest_frame()
            if frame_data:
                print(f"âœ“ è·å–åˆ°ä¸€å¸§å›¾åƒï¼Œæ—¶é—´æˆ³: {frame_data['timestamp']:.2f}")
                print(f"  å›¾åƒå°ºå¯¸: 416x416")
                print(f"  æ£€æµ‹ç‰©ä½“æ•°: {len(frame_data['objects'])}")
            else:
                print("âœ— æœªèƒ½è·å–å›¾åƒå¸§")
            
            camera.stop_capture()
            
        except Exception as e:
            print(f"âœ— è§†è§‰æ„ŸçŸ¥æ¼”ç¤ºå¤±è´¥: {e}")
        
        # 2. éŸ³é¢‘æ„ŸçŸ¥æ¼”ç¤º
        print("\n2. éŸ³é¢‘æ„ŸçŸ¥æ¨¡å—æ¼”ç¤º")
        try:
            audio = AudioPerception()
            audio.start_recording()
            
            print("éŸ³é¢‘å½•åˆ¶å¯åŠ¨ä¸­...")
            time.sleep(2)  # ç­‰å¾…éŸ³é¢‘ç¨³å®š
            
            audio_data = audio.get_latest_audio()
            if audio_data:
                print(f"âœ“ è·å–åˆ°éŸ³é¢‘æ•°æ®ï¼Œæ—¶é—´æˆ³: {audio_data['timestamp']:.2f}")
                print(f"  éŸ³é¢‘é•¿åº¦: {len(audio_data['audio'])} æ ·æœ¬")
                print(f"  é‡‡æ ·ç‡: {audio_data['sample_rate']} Hz")
                
                # æµ‹è¯•è¯­éŸ³è¯†åˆ«
                if audio.whisper_model:
                    transcription = audio.transcribe_audio(
                        audio_data['audio'], 
                        audio_data['sample_rate']
                    )
                    print(f"  è¯†åˆ«æ–‡æœ¬: '{transcription['text']}'")
                    print(f"  è¯†åˆ«è¯­è¨€: {transcription['language']}")
            else:
                print("âœ— æœªèƒ½è·å–éŸ³é¢‘æ•°æ®")
            
            audio.stop_recording()
            
        except Exception as e:
            print(f"âœ— éŸ³é¢‘æ„ŸçŸ¥æ¼”ç¤ºå¤±è´¥: {e}")
        
        # 3. ç©ºé—´æ„ŸçŸ¥æ¼”ç¤º
        print("\n3. ç©ºé—´æ„ŸçŸ¥æ¨¡å—æ¼”ç¤º")
        try:
            spatial = SpatialPerception(num_points=1000)
            
            # æ¨¡æ‹Ÿç‚¹äº‘æ•°æ®
            points = spatial._simulate_lidar_data()
            print(f"âœ“ ç”Ÿæˆäº† {len(points)} ä¸ªç‚¹äº‘ç‚¹")
            
            # å¤„ç†ç‚¹äº‘
            pcd = spatial._process_point_cloud(points)
            print(f"âœ“ å¤„ç†åç‚¹äº‘åŒ…å« {len(pcd.points)} ä¸ªç‚¹")
            
            # æå–ç©ºé—´ç‰¹å¾
            features = spatial.extract_spatial_features(pcd)
            print(f"âœ“ æå–åˆ° {len(features)} ä¸ªç©ºé—´ç‰¹å¾")
            
            for i, feature in enumerate(features[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"  ç‰¹å¾ {i+1}:")
                print(f"    è´¨å¿ƒ: {feature.centroid}")
                print(f"    ä½“ç§¯: {feature.volume:.2f}")
                print(f"    è¡¨é¢ç§¯: {feature.surface_area:.2f}")
        
        except Exception as e:
            print(f"âœ— ç©ºé—´æ„ŸçŸ¥æ¼”ç¤ºå¤±è´¥: {e}")
    
    def demo_world_model(self):
        """æ¼”ç¤ºä¸–ç•Œæ¨¡å‹"""
        self.print_header("ä¸–ç•Œæ¨¡å‹åŠ¨æ€æ„å»ºæ¼”ç¤º")
        
        try:
            world = WorldModel()
            print("ä¸–ç•Œæ¨¡å‹å·²åˆå§‹åŒ–")
            
            # æ¨¡æ‹Ÿè§†è§‰æ•°æ®è¾“å…¥
            print("\nè¾“å…¥æ¨¡æ‹Ÿè§†è§‰æ•°æ®...")
            visual_data = {
                'timestamp': time.time(),
                'modality': 'visual',
                'data': {
                    'objects': [
                        {'class': 'person', 'confidence': 0.9, 'bbox': [100, 50, 80, 120]},
                        {'class': 'car', 'confidence': 0.8, 'bbox': [300, 100, 150, 80]}
                    ]
                },
                'confidence': 0.85
            }
            
            from core.perception.multimodal_sensing import PerceptionData
            visual_perception = PerceptionData(**visual_data)
            world.update_world_state([visual_perception])
            
            print(f"âœ“ ä¸–ç•Œä¸­å½“å‰æœ‰ {len(world.objects)} ä¸ªå¯¹è±¡")
            
            # æ˜¾ç¤ºä¸–ç•ŒçŠ¶æ€
            current_state = world.get_current_state()
            print("\nå½“å‰ä¸–ç•ŒçŠ¶æ€:")
            self.print_data("å¯¹è±¡", {obj_id: {
                'ä½ç½®': obj.position.tolist(),
                'ç½®ä¿¡åº¦': obj.confidence,
                'æœ€åçœ‹è§': obj.last_seen
            } for obj_id, obj in current_state['objects'].items()})
            
            # æ¨¡æ‹Ÿç©ºé—´æ•°æ®è¾“å…¥
            print("\nè¾“å…¥æ¨¡æ‹Ÿç©ºé—´æ•°æ®...")
            spatial_data = {
                'timestamp': time.time(),
                'modality': 'spatial',
                'data': {
                    'features': [
                        {
                            'centroid': [200, 100, 50],
                            'volume': 1000.0,
                            'surface_area': 200.0,
                            'bounds': np.array([[0, 0, 0], [400, 200, 100]])
                        }
                    ]
                },
                'confidence': 0.7
            }
            
            # åˆ›å»ºç©ºé—´ç‰¹å¾å¯¹è±¡
            from core.perception.multimodal_sensing import SpatialFeature
            feature = SpatialFeature(
                centroid=np.array([200, 100, 50]),
                bounds=np.array([[0, 0, 0], [400, 200, 100]]),
                surface_area=200.0,
                volume=1000.0,
                orientation=np.array([1, 0, 0])
            )
            
            spatial_perception = PerceptionData(
                timestamp=time.time(),
                modality='spatial',
                data={'features': [feature]},
                confidence=0.7
            )
            world.update_world_state([spatial_perception])
            
            print(f"âœ“ ä¸–ç•Œä¸­ç°åœ¨æœ‰ {len(world.objects)} ä¸ªå¯¹è±¡")
            
        except Exception as e:
            print(f"âœ— ä¸–ç•Œæ¨¡å‹æ¼”ç¤ºå¤±è´¥: {e}")
    
    def demo_multimodal_fusion(self):
        """æ¼”ç¤ºå¤šæ¨¡æ€èåˆ"""
        self.print_header("å¤šæ¨¡æ€èåˆæ¼”ç¤º")
        
        try:
            fusion = MultimodalFusion(feature_dim=64)
            print("å¤šæ¨¡æ€èåˆå¼•æ“å·²åˆå§‹åŒ–")
            
            # åˆ›å»ºæ¨¡æ‹Ÿæ„ŸçŸ¥æ•°æ®
            visual_data = PerceptionData(
                timestamp=time.time(),
                modality='visual',
                data={'frame': np.random.rand(224, 224, 3)},
                confidence=0.8
            )
            
            audio_data = PerceptionData(
                timestamp=time.time(),
                modality='audio',
                data={'audio': np.random.rand(16000), 'sample_rate': 16000},
                confidence=0.7
            )
            
            spatial_data = PerceptionData(
                timestamp=time.time(),
                modality='spatial',
                data={'features': []},
                confidence=0.6
            )
            
            perception_list = [visual_data, audio_data, spatial_data]
            
            # æ‰§è¡Œç‰¹å¾èåˆ
            print("\næ‰§è¡Œå¤šæ¨¡æ€ç‰¹å¾èåˆ...")
            fused_features = fusion.extract_fused_features(perception_list)
            
            print(f"âœ“ æˆåŠŸæå–èåˆç‰¹å¾")
            print(f"  ç‰¹å¾ç»´åº¦: {len(fused_features)}")
            print(f"  ç‰¹å¾å‡å€¼: {np.mean(fused_features):.4f}")
            print(f"  ç‰¹å¾æ ‡å‡†å·®: {np.std(fused_features):.4f}")
            
            # æµ‹è¯•ç‰¹å¾ç›¸ä¼¼åº¦
            features1 = np.random.rand(64)
            features2 = np.random.rand(64)
            similarity = fusion.compute_similarity(features1, features2)
            
            print(f"\nâœ“ ç‰¹å¾ç›¸ä¼¼åº¦æµ‹è¯•")
            print(f"  ç›¸ä¼¼åº¦: {similarity:.4f}")
            
        except Exception as e:
            print(f"âœ— å¤šæ¨¡æ€èåˆæ¼”ç¤ºå¤±è´¥: {e}")
    
    def demo_integrated_system(self, duration: int = 30):
        """æ¼”ç¤ºé›†æˆç³»ç»Ÿ"""
        self.print_header(f"é›†æˆç³»ç»Ÿæ¼”ç¤º ({duration}ç§’)")
        
        try:
            self.system = MultimodalSensingSystem(self.config)
            self.is_demo_running = True
            
            print("å¯åŠ¨å¤šæ¨¡æ€æ„ŸçŸ¥ç³»ç»Ÿ...")
            self.system.start_system()
            
            # åˆ›å»ºç›‘æ§çº¿ç¨‹
            monitor_thread = threading.Thread(
                target=self._monitor_system,
                args=(duration,)
            )
            monitor_thread.start()
            
            # ç­‰å¾…æ¼”ç¤ºç»“æŸ
            monitor_thread.join()
            
            print("\nâœ“ é›†æˆç³»ç»Ÿæ¼”ç¤ºå®Œæˆ")
            
        except Exception as e:
            print(f"âœ— é›†æˆç³»ç»Ÿæ¼”ç¤ºå¤±è´¥: {e}")
        
        finally:
            if self.system:
                self.system.stop_system()
                self.is_demo_running = False
    
    def _monitor_system(self, duration: int):
        """ç›‘æ§ç³»ç»Ÿå’Œæ˜¾ç¤ºç»“æœ"""
        start_time = time.time()
        update_interval = 2  # æ¯2ç§’æ›´æ–°ä¸€æ¬¡
        
        while (time.time() - start_time) < duration and self.is_demo_running:
            try:
                # è·å–æœ€æ–°æ„ŸçŸ¥æ•°æ®
                perception = self.system.get_latest_perception()
                
                if perception:
                    world_state = perception['world_state']
                    stats = perception['stats']
                    fused_features = perception['fused_features']
                    
                    print(f"\n--- {time.strftime('%H:%M:%S')} ---")
                    print(f"æ£€æµ‹å¯¹è±¡æ•°: {world_state['num_objects']}")
                    print(f"å¤„ç†å¸§æ•°: {stats['frame_count']}")
                    print(f"éŸ³é¢‘æ®µæ•°: {stats['audio_count']}")
                    print(f"ç‚¹äº‘æ•°: {stats['point_cloud_count']}")
                    print(f"èåˆæ¬¡æ•°: {stats['fusion_count']}")
                    print(f"ç‰¹å¾ç»´åº¦: {len(fused_features)}")
                    
                    # æ˜¾ç¤ºæœ€æ–°éŸ³é¢‘
                    if world_state['current_state'].get('last_audio'):
                        audio_text = world_state['current_state']['last_audio']['text']
                        if audio_text:
                            print(f"æœ€æ–°è¯­éŸ³: '{audio_text}'")
                    
                    # æ˜¾ç¤ºéƒ¨åˆ†ç‰¹å¾å€¼
                    if fused_features:
                        print(f"ç‰¹å¾æ ·æœ¬: {[f'{x:.3f}' for x in fused_features[:5]]}")
                
                time.sleep(update_interval)
            
            except Exception as e:
                print(f"ç›‘æ§é”™è¯¯: {e}")
                break
    
    def run_interactive_demo(self):
        """è¿è¡Œäº¤äº’å¼æ¼”ç¤º"""
        self.print_header("äº¤äº’å¼å¤šæ¨¡æ€æ„ŸçŸ¥æ¼”ç¤º")
        
        print("é€‰æ‹©æ¼”ç¤ºæ¨¡å¼:")
        print("1. å•æ¨¡æ€æ¼”ç¤º")
        print("2. ä¸–ç•Œæ¨¡å‹æ¼”ç¤º")
        print("3. å¤šæ¨¡æ€èåˆæ¼”ç¤º")
        print("4. é›†æˆç³»ç»Ÿæ¼”ç¤º (30ç§’)")
        print("5. å®Œæ•´æ¼”ç¤º (æ‰€æœ‰æ¨¡å¼)")
        print("0. é€€å‡º")
        
        try:
            choice = input("\nè¯·é€‰æ‹© (0-5): ").strip()
            
            if choice == '1':
                self.demo_single_modality()
            elif choice == '2':
                self.demo_world_model()
            elif choice == '3':
                self.demo_multimodal_fusion()
            elif choice == '4':
                duration = input("æ¼”ç¤ºæ—¶é•¿ (ç§’ï¼Œé»˜è®¤30): ").strip()
                try:
                    duration = int(duration) if duration else 30
                except:
                    duration = 30
                self.demo_integrated_system(duration)
            elif choice == '5':
                print("\nå¼€å§‹å®Œæ•´æ¼”ç¤º...")
                self.demo_single_modality()
                input("\næŒ‰å›è½¦ç»§ç»­åˆ°ä¸–ç•Œæ¨¡å‹æ¼”ç¤º...")
                self.demo_world_model()
                input("\næŒ‰å›è½¦ç»§ç»­åˆ°å¤šæ¨¡æ€èåˆæ¼”ç¤º...")
                self.demo_multimodal_fusion()
                input("\næŒ‰å›è½¦ç»§ç»­åˆ°é›†æˆç³»ç»Ÿæ¼”ç¤º...")
                self.demo_integrated_system(30)
                print("\nğŸ‰ å®Œæ•´æ¼”ç¤ºç»“æŸï¼")
            elif choice == '0':
                print("é€€å‡ºæ¼”ç¤º")
                return
            else:
                print("æ— æ•ˆé€‰æ‹©")
        
        except KeyboardInterrupt:
            print("\n\næ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            print(f"\næ¼”ç¤ºæ‰§è¡Œé”™è¯¯: {e}")
    
    def run_batch_demo(self):
        """è¿è¡Œæ‰¹é‡æ¼”ç¤º"""
        self.print_header("è‡ªåŠ¨æ‰¹é‡æ¼”ç¤º")
        
        print("è‡ªåŠ¨æ‰§è¡Œæ‰€æœ‰æ¼”ç¤ºæ¨¡å¼...")
        
        self.demo_single_modality()
        time.sleep(2)
        
        self.demo_world_model()
        time.sleep(2)
        
        self.demo_multimodal_fusion()
        time.sleep(2)
        
        self.demo_integrated_system(10)  # è¾ƒçŸ­çš„é›†æˆæ¼”ç¤º
        
        print("\nğŸ‰ æ‰¹é‡æ¼”ç¤ºå®Œæˆï¼")


def main():
    """ä¸»å‡½æ•°"""
    try:
        demo = MultimodalDemo()
        
        print("å¤šæ¨¡æ€ä¸–ç•Œæ¨¡å‹æ„ŸçŸ¥ç³»ç»Ÿæ¼”ç¤º")
        print("=" * 50)
        print("æ”¯æŒçš„åŠŸèƒ½:")
        print("- è§†è§‰æ„ŸçŸ¥ (USBæ‘„åƒå¤´ + ç‰©ä½“è¯†åˆ«)")
        print("- éŸ³é¢‘æ„ŸçŸ¥ (Whisperè¯­éŸ³è¯†åˆ«)")
        print("- ç©ºé—´æ„ŸçŸ¥ (æ¿€å…‰é›·è¾¾ç‚¹äº‘å¤„ç†)")
        print("- ä¸–ç•Œæ¨¡å‹åŠ¨æ€æ„å»º")
        print("- å¤šæ¨¡æ€ç‰¹å¾èåˆ")
        
        mode = input("\né€‰æ‹©è¿è¡Œæ¨¡å¼ (1: äº¤äº’å¼, 2: æ‰¹é‡): ").strip()
        
        if mode == '1':
            demo.run_interactive_demo()
        elif mode == '2':
            demo.run_batch_demo()
        else:
            print("ä½¿ç”¨äº¤äº’å¼æ¨¡å¼")
            demo.run_interactive_demo()
        
    except KeyboardInterrupt:
        print("\næ¼”ç¤ºè¢«ä¸­æ–­")
    except Exception as e:
        print(f"\næ¼”ç¤ºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()