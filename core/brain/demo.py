#!/usr/bin/env python3
"""
æµ·é©¬ä½“è®°å¿†ç³»ç»Ÿæ¼”ç¤ºè„šæœ¬
å±•ç¤ºå®Œæ•´çš„åŠŸèƒ½ç‰¹æ€§
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'brain'))

from hippocampus import *

def demonstrate_hippocampus_system():
    """æ¼”ç¤ºæµ·é©¬ä½“è®°å¿†ç³»ç»Ÿçš„å®Œæ•´åŠŸèƒ½"""
    
    print("ğŸ§  æµ·é©¬ä½“è®°å¿†ç³»ç»ŸåŠŸèƒ½æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºç³»ç»Ÿå®ä¾‹
    memory_system = HippocampusMemorySystem(
        max_memory_size=500,
        embedding_dim=64,
        consolidation_hour=22
    )
    
    # 1. å­˜å‚¨å¤šæ ·åŒ–çš„è®°å¿†
    print("\n1. ğŸ“ å­˜å‚¨å¤šæ ·åŒ–è®°å¿†")
    
    memories_data = [
        ("å­¦ä¼šäº†Pythonç¼–ç¨‹", "semantic", 0.8, 0.6, False),
        ("å®Œæˆäº†ä¸€ä¸ªé‡è¦é¡¹ç›®", "episodic", 0.9, 0.7, True),
        ("å’Œæœ‹å‹ä»¬èšé¤å¾ˆå¼€å¿ƒ", "episodic", 0.7, 0.8, False),
        ("å‘ç°äº†æ–°çš„ç®—æ³•", "creative", 0.8, 0.6, True),
        ("å·¥ä½œä¸­é‡åˆ°æŒ«æŠ˜", "episodic", -0.3, -0.2, False),
        ("æŒæ¡äº†æœºå™¨å­¦ä¹ ", "semantic", 0.9, 0.7, False),
        ("åˆ›é€ äº†æœ‰è¶£çš„åº”ç”¨", "creative", 0.9, 0.8, True),
        ("å›¢é˜Ÿåä½œæˆåŠŸ", "episodic", 0.8, 0.7, False),
        ("å­¦ä¹ äº†æ·±åº¦å­¦ä¹ ", "semantic", 0.8, 0.6, False),
        ("è§£å†³äº†éš¾é¢˜å¾ˆå…´å¥‹", "creative", 0.8, 0.9, True)
    ]
    
    stored_ids = []
    for i, (content, mem_type, reward, emotion, creativity) in enumerate(memories_data):
        memory_id = memory_system.store_memory(
            content=content,
            memory_type=mem_type,
            reward_value=reward,
            emotional_valence=emotion,
            creativity_flag=creativity
        )
        stored_ids.append(memory_id)
        print(f"   {i+1:2d}. {content}")
    
    # ç­‰å¾…å¼‚æ­¥å¤„ç†
    time.sleep(2)
    
    # 2. æ¦‚å¿µå½¢æˆæ¼”ç¤º
    print(f"\n2. ğŸ§  æ¦‚å¿µå½¢æˆæ¼”ç¤º")
    
    # è§¦å‘æ¦‚å¿µå½¢æˆ
    programming_ids = stored_ids[:3]  # é€‰æ‹©ç¼–ç¨‹ç›¸å…³è®°å¿†
    concepts = memory_system.form_concepts_from_memories(programming_ids)
    
    print(f"   ä» {len(programming_ids)} ä¸ªç¼–ç¨‹è®°å¿†å½¢æˆæ¦‚å¿µ:")
    for concept_id in concepts:
        concept = memory_system.concepts[concept_id]
        print(f"   - {concept.name}")
        print(f"     å®šä¹‰: {concept.definition}")
        print(f"     å±æ€§: {', '.join(list(concept.attributes)[:3])}")
        print(f"     ç½®ä¿¡åº¦: {concept.confidence_score:.3f}")
    
    # 3. çŸ¥è¯†è’¸é¦æ¼”ç¤º
    print(f"\n3. ğŸ¯ çŸ¥è¯†è’¸é¦æ¼”ç¤º")
    
    # é€‰æ‹©å¤šä¸ªè®°å¿†è¿›è¡Œè’¸é¦
    multiple_ids = stored_ids[:5]
    knowledge_id = memory_system.distill_knowledge(multiple_ids)
    
    if knowledge_id:
        knowledge = memory_system.distilled_knowledge[knowledge_id]
        print(f"   è’¸é¦çŸ¥è¯†æˆåŠŸ:")
        print(f"     ID: {knowledge.knowledge_id[:8]}...")
        print(f"     å‹ç¼©æ¯”: {knowledge.compression_ratio:.2f}")
        print(f"     ä¿çœŸåº¦: {knowledge.fidelity_score:.3f}")
        print(f"     è´¨é‡åˆ†æ•°: {knowledge.quality_score:.3f}")
        print(f"     å…³é”®ç‰¹å¾æ•°: {len(knowledge.key_features)}")
        
        # æ˜¾ç¤ºå…³é”®ç‰¹å¾
        print("     ä¸»è¦ç‰¹å¾:")
        for feature, value in list(knowledge.key_features.items())[:3]:
            print(f"       - {feature}: {value:.3f}")
    else:
        print("   çŸ¥è¯†è’¸é¦å¤±è´¥")
    
    # 4. è¯­ä¹‰ç½‘ç»œæ¼”ç¤º
    print(f"\n4. ğŸŒ è¯­ä¹‰ç½‘ç»œæ¼”ç¤º")
    
    # æ„å»ºè¯­ä¹‰ç½‘ç»œ
    memory_system.build_semantic_network()
    
    print(f"   è¯­ä¹‰ç½‘ç»œç»Ÿè®¡:")
    print(f"     æ¦‚å¿µæ•°: {len(memory_system.concepts)}")
    print(f"     å…³è”æ•°: {sum(len(connections) for connections in memory_system.semantic_network.edges.values()) // 2}")
    
    # å±•ç¤ºæ¦‚å¿µå…³ç³»
    if memory_system.concepts:
        first_concept = list(memory_system.concepts.values())[0]
        relationships = memory_system.find_semantic_relationships(first_concept.concept_id)
        
        print(f"   æ¦‚å¿µ '{first_concept.name}' çš„è¯­ä¹‰å…³ç³»:")
        for rel in relationships[:3]:
            print(f"     - {rel['description']}")
    
    # 5. è®°å¿†æ£€ç´¢æ¼”ç¤º
    print(f"\n5. ğŸ” è®°å¿†æ£€ç´¢æ¼”ç¤º")
    
    # ç²¾ç¡®æ£€ç´¢
    query_results = memory_system.retrieve_memories("ç¼–ç¨‹å­¦ä¹ ", top_k=3)
    print(f"   ç²¾ç¡®æ£€ç´¢ 'ç¼–ç¨‹å­¦ä¹ ': {len(query_results)} ä¸ªç»“æœ")
    for i, result in enumerate(query_results):
        memory = result['memory']
        print(f"     {i+1}. {memory.content}")
        print(f"        ç›¸ä¼¼åº¦: {result['similarity_score']:.3f}")
        print(f"        å…³è”è®°å¿†æ•°: {len(result['related_memories'])}")
    
    # 6. é•¿æœŸå·©å›ºæ¼”ç¤º
    print(f"\n6. ğŸ’¾ é•¿æœŸå·©å›ºæ¼”ç¤º")
    
    # å¼ºåˆ¶æ‰§è¡Œå·©å›º
    consolidation_result = memory_system.consolidate_memories(force=True)
    
    print(f"   å·©å›ºç»“æœ:")
    print(f"     çŠ¶æ€: {consolidation_result['status']}")
    print(f"     å·©å›ºè®°å¿†æ•°: {consolidation_result['consolidated_memories']}")
    print(f"     é—å¿˜è®°å¿†æ•°: {consolidation_result['forgotten_memories']}")
    print(f"     æ–°æ¦‚å¿µæ•°: {consolidation_result['new_concepts']}")
    print(f"     æ–°è’¸é¦çŸ¥è¯†æ•°: {consolidation_result['new_distilled_knowledge']}")
    print(f"     å¤„ç†æ—¶é—´: {consolidation_result['processing_time']:.3f}ç§’")
    
    # 7. ç³»ç»Ÿç»Ÿè®¡æ¼”ç¤º
    print(f"\n7. ğŸ“Š ç³»ç»Ÿç»Ÿè®¡æ¼”ç¤º")
    
    stats = memory_system.get_memory_statistics()
    
    print(f"   è®°å¿†æ¦‚è§ˆ:")
    print(f"     æ€»è®°å¿†æ•°: {stats['memory_overview']['total_memories']}")
    print(f"     å®¹é‡ä½¿ç”¨ç‡: {stats['memory_overview']['memory_capacity_usage']:.1%}")
    print(f"     å·¥ä½œè®°å¿†å¤§å°: {stats['memory_overview']['working_memory_size']}")
    
    print(f"   è®°å¿†åˆ†å¸ƒ:")
    for mem_type, count in stats['memory_distribution']['by_type'].items():
        print(f"     {mem_type}: {count}")
    
    print(f"   æ¦‚å¿µç»Ÿè®¡:")
    print(f"     æ¦‚å¿µæ•°: {stats['conceptual_stats']['total_concepts']}")
    print(f"     å½¢æˆæ¦‚å¿µæ•°: {stats['conceptual_stats']['concepts_formed']}")
    print(f"     è¯­ä¹‰å…³è”æ•°: {stats['conceptual_stats']['semantic_network']['total_associations']}")
    
    print(f"   çŸ¥è¯†è’¸é¦ç»Ÿè®¡:")
    print(f"     è’¸é¦çŸ¥è¯†æ•°: {stats['knowledge_stats']['total_distilled_knowledge']}")
    print(f"     å¹³å‡å‹ç¼©æ¯”: {stats['knowledge_stats']['avg_compression_ratio']:.2f}")
    print(f"     å¹³å‡è´¨é‡åˆ†: {stats['knowledge_stats']['avg_quality_score']:.3f}")
    
    print(f"   æ€§èƒ½ç»Ÿè®¡:")
    print(f"     æ£€ç´¢å‡†ç¡®ç‡: {stats['performance_stats']['retrieval_accuracy']:.1%}")
    print(f"     æˆåŠŸæ£€ç´¢: {stats['performance_stats']['successful_retrievals']}")
    print(f"     å¤±è´¥æ£€ç´¢: {stats['performance_stats']['failed_retrievals']}")
    
    # 8. æ•°æ®æŒä¹…åŒ–æ¼”ç¤º
    print(f"\n8. ğŸ’¾ æ•°æ®æŒä¹…åŒ–æ¼”ç¤º")
    
    # å¯¼å‡ºè®°å¿†çŠ¶æ€
    export_file = "memory_state.json"
    memory_system.export_memory_state(export_file)
    print(f"   è®°å¿†çŠ¶æ€å·²å¯¼å‡ºåˆ°: {export_file}")
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    if os.path.exists(export_file):
        file_size = os.path.getsize(export_file)
        print(f"   æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
    
    # æ¸…ç†èµ„æº
    memory_system.cleanup()
    print(f"   èµ„æºæ¸…ç†å®Œæˆ")
    
    print(f"\n" + "=" * 60)
    print(f"âœ… æµ·é©¬ä½“è®°å¿†ç³»ç»ŸåŠŸèƒ½æ¼”ç¤ºå®Œæˆ!")
    print(f"")
    print(f"ğŸ¯ æ ¸å¿ƒåŠŸèƒ½éªŒè¯:")
    print(f"   âœ… æ¦‚å¿µå½¢æˆå’ŒæŠ½è±¡åŒ–æœºåˆ¶")
    print(f"   âœ… çŸ¥è¯†è’¸é¦å’Œå‹ç¼©å­˜å‚¨")
    print(f"   âœ… è¯­ä¹‰è®°å¿†ç½‘ç»œ")
    print(f"   âœ… è®°å¿†æå–å’Œå…³è”")
    print(f"   âœ… é•¿æœŸè®°å¿†å·©å›º")
    print(f"")
    print(f"ğŸš€ ç³»ç»Ÿå·²å‡†å¤‡å¥½é›†æˆåˆ°æ›´å¤§çš„AIåº”ç”¨ä¸­!")
    print(f"=" * 60)
    
    return memory_system

if __name__ == "__main__":
    demonstrate_hippocampus_system()