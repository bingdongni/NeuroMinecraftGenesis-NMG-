"""
æµ·é©¬ä½“è®°å¿†ç³»ç»Ÿ - è®°å¿†å¤„ç†æ ¸å¿ƒæ¨¡å—
è´Ÿè´£æ¦‚å¿µå½¢æˆã€çŸ¥è¯†è’¸é¦ã€è¯­ä¹‰ç½‘ç»œã€è®°å¿†æå–å’Œé•¿æœŸå·©å›º
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import time
import json
from typing import Dict, List, Tuple, Set, Any, Optional, Union
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
import math
import logging
from concurrent.futures import ThreadPoolExecutor
import threading
import uuid

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class Memory:
    """è®°å¿†å•å…ƒç»“æ„"""
    memory_id: str
    content: Any
    vector_embedding: Optional[np.ndarray]  # å‘é‡è¡¨ç¤º
    concept_level: int  # æ¦‚å¿µæŠ½è±¡å±‚çº§ (0-5)
    semantic_tags: Set[str]  # è¯­ä¹‰æ ‡ç­¾
    associations: Set[str]  # å…³è”è®°å¿†ID
    timestamp: float
    strength: float  # è®°å¿†å¼ºåº¦ (0-1)
    access_count: int  # è®¿é—®æ¬¡æ•°
    consolidation_level: int  # å·©å›ºå±‚çº§ (0-5)
    memory_type: str = "episodic"  # è®°å¿†ç±»å‹
    reward_value: float = 0.0  # å¥–åŠ±å€¼
    emotional_valence: float = 0.0  # æƒ…æ„Ÿææ€§
    creativity_flag: bool = False  # åˆ›é€ åŠ›æ ‡è®°


@dataclass
class Concept:
    """æ¦‚å¿µç»“æ„"""
    concept_id: str
    name: str
    definition: str
    attributes: Set[str]
    examples: List[str]
    abstraction_level: int  # æŠ½è±¡å±‚çº§
    prototype_embedding: np.ndarray  # åŸå‹å‘é‡
    constituent_memories: List[str]  # ç»„æˆè®°å¿†ID
    related_concepts: Set[str]  # ç›¸å…³æ¦‚å¿µID
    formation_time: float
    confidence_score: float = 1.0


@dataclass
class DistilledKnowledge:
    """è’¸é¦çŸ¥è¯†ç»“æ„"""
    knowledge_id: str
    original_memory_ids: List[str]
    compressed_embedding: np.ndarray  # å‹ç¼©åçš„å‘é‡
    key_features: Dict[str, float]  # å…³é”®ç‰¹å¾
    compression_ratio: float  # å‹ç¼©æ¯”ä¾‹
    fidelity_score: float  # ä¿çœŸåº¦åˆ†æ•°
    formation_time: float
    quality_score: float  # è´¨é‡åˆ†æ•°


class SemanticNetwork:
    """è¯­ä¹‰è®°å¿†ç½‘ç»œ"""
    
    def __init__(self, embedding_dim: int = 256):
        self.nodes: Dict[str, Concept] = {}
        self.edges: Dict[str, Dict[str, float]] = defaultdict(dict)
        self.categories: Dict[str, Set[str]] = defaultdict(set)
        self.embedding_dim = embedding_dim
        self.hierarchy_levels = 6
        
    def add_concept(self, concept: Concept):
        """æ·»åŠ æ¦‚å¿µåˆ°è¯­ä¹‰ç½‘ç»œ"""
        self.nodes[concept.concept_id] = concept
        
    def add_association(self, concept1: str, concept2: str, strength: float = 1.0):
        """æ·»åŠ æ¦‚å¿µé—´å…³è”"""
        self.edges[concept1][concept2] = strength
        self.edges[concept2][concept1] = strength
        
    def get_related_concepts(self, concept_id: str, threshold: float = 0.5) -> List[Tuple[str, float]]:
        """è·å–ç›¸å…³æ¦‚å¿µ"""
        if concept_id not in self.edges:
            return []
        
        related = [(other, strength) for other, strength in self.edges[concept_id].items() 
                  if strength >= threshold]
        return sorted(related, key=lambda x: x[1], reverse=True)
    
    def build_semantic_hierarchy(self) -> Dict[int, Set[str]]:
        """æ„å»ºè¯­ä¹‰å±‚æ¬¡ç»“æ„"""
        hierarchy = defaultdict(set)
        
        for concept in self.nodes.values():
            hierarchy[concept.abstraction_level].add(concept.concept_id)
        
        return dict(hierarchy)
    
    def compute_concept_similarity(self, concept1_id: str, concept2_id: str) -> float:
        """è®¡ç®—æ¦‚å¿µç›¸ä¼¼åº¦"""
        if concept1_id not in self.nodes or concept2_id not in self.nodes:
            return 0.0
        
        concept1 = self.nodes[concept1_id]
        concept2 = self.nodes[concept2_id]
        
        # å±æ€§é‡å åº¦
        attr_overlap = len(concept1.attributes & concept2.attributes)
        attr_union = len(concept1.attributes | concept2.attributes)
        attr_similarity = attr_overlap / max(attr_union, 1)
        
        # æŠ½è±¡å±‚çº§ç›¸ä¼¼åº¦
        level_diff = abs(concept1.abstraction_level - concept2.abstraction_level)
        level_similarity = 1.0 / (1.0 + level_diff)
        
        # åµŒå…¥å‘é‡ç›¸ä¼¼åº¦
        if concept1.prototype_embedding is not None and concept2.prototype_embedding is not None:
            vec_similarity = np.dot(concept1.prototype_embedding, concept2.prototype_embedding) / (
                np.linalg.norm(concept1.prototype_embedding) * np.linalg.norm(concept2.prototype_embedding) + 1e-8
            )
        else:
            vec_similarity = 0.0
        
        # ç»¼åˆç›¸ä¼¼åº¦
        total_similarity = (attr_similarity * 0.4 + level_similarity * 0.3 + vec_similarity * 0.3)
        return max(0.0, total_similarity)


class ConceptFormationNetwork(nn.Module):
    """æ¦‚å¿µå½¢æˆç½‘ç»œ"""
    
    def __init__(self, input_dim: int = 256, hidden_dim: int = 128):
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        
        # æ¦‚å¿µç¼–ç å™¨
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, input_dim)
        )
        
        # æŠ½è±¡åŒ–ç½‘ç»œ
        self.abstraction_net = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 6)  # 6ä¸ªæŠ½è±¡å±‚çº§
        )
        
        # æ¦‚å¿µåˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(input_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 10)  # 10ç§æ¦‚å¿µç±»å‹
        )
    
    def forward(self, embeddings: torch.Tensor) -> Dict[str, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
        # æ¦‚å¿µç¼–ç 
        encoded = self.encoder(embeddings)
        
        # æŠ½è±¡çº§åˆ«é¢„æµ‹
        abstraction_probs = F.softmax(self.abstraction_net(encoded), dim=-1)
        
        # æ¦‚å¿µç±»å‹é¢„æµ‹
        type_probs = F.softmax(self.classifier(encoded), dim=-1)
        
        return {
            'encoded_concepts': encoded,
            'abstraction_probs': abstraction_probs,
            'type_probs': type_probs
        }
    
    def extract_prototype(self, embeddings: torch.Tensor) -> torch.Tensor:
        """æå–åŸå‹å‘é‡"""
        return torch.mean(embeddings, dim=0)


class KnowledgeDistiller(nn.Module):
    """çŸ¥è¯†è’¸é¦å™¨"""
    
    def __init__(self, input_dim: int = 256, compression_ratio: float = 0.5):
        super().__init__()
        self.input_dim = input_dim
        self.compressed_dim = int(input_dim * compression_ratio)
        
        # æ³¨æ„åŠ›èšåˆå™¨
        self.attention = nn.MultiheadAttention(
            embed_dim=input_dim,
            num_heads=8,
            dropout=0.1
        )
        
        # ç‰¹å¾æå–å™¨
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 32)
        )
        
        # å‹ç¼©å™¨
        self.compressor = nn.Sequential(
            nn.Linear(input_dim, self.compressed_dim),
            nn.ReLU(),
            nn.Linear(self.compressed_dim, input_dim)
        )
        
        # è´¨é‡è¯„ä¼°å™¨
        self.quality_assessor = nn.Sequential(
            nn.Linear(input_dim, 16),
            nn.ReLU(),
            nn.Linear(16, 2)  # ä¿çœŸåº¦ + æ³›åŒ–èƒ½åŠ›
        )
    
    def forward(self, memory_embeddings: torch.Tensor) -> Dict[str, torch.Tensor]:
        """çŸ¥è¯†è’¸é¦è¿‡ç¨‹"""
        batch_size = memory_embeddings.size(0)
        
        # æ³¨æ„åŠ›èšåˆ
        attended, attention_weights = self.attention(
            memory_embeddings, memory_embeddings, memory_embeddings
        )
        
        # å…¨å±€è¡¨ç¤º
        global_repr = attended.mean(dim=0)
        
        # ç‰¹å¾æå–
        features = self.feature_extractor(global_repr)
        
        # å‹ç¼©è¡¨ç¤º
        compressed = self.compressor(global_repr)
        
        # è´¨é‡è¯„ä¼°
        quality_scores = self.quality_assessor(global_repr)
        
        return {
            'compressed_embedding': compressed,
            'attention_weights': attention_weights,
            'fidelity_score': quality_scores[0],
            'generalization_score': quality_scores[1],
            'features': features
        }


class HippocampusMemorySystem:
    """æµ·é©¬ä½“è®°å¿†ç³»ç»Ÿæ ¸å¿ƒç±»"""
    
    def __init__(self, 
                 max_memory_size: int = 10000,
                 embedding_dim: int = 256,
                 consolidation_hour: int = 22):
        # æ ¸å¿ƒå‚æ•°
        self.max_memory_size = max_memory_size
        self.embedding_dim = embedding_dim
        self.consolidation_hour = consolidation_hour
        
        # è®°å¿†å­˜å‚¨
        self.memories: Dict[str, Memory] = {}
        self.semantic_network = SemanticNetwork(embedding_dim)
        
        # æ¦‚å¿µå’ŒçŸ¥è¯†å­˜å‚¨
        self.concepts: Dict[str, Concept] = {}
        self.distilled_knowledge: Dict[str, DistilledKnowledge] = {}
        
        # ç¥ç»ç½‘ç»œç»„ä»¶
        self.concept_network = ConceptFormationNetwork(embedding_dim)
        self.knowledge_distiller = KnowledgeDistiller(embedding_dim)
        
        # å·¥ä½œè®°å¿†ç¼“å†²åŒº
        self.working_memory: deque = deque(maxlen=7)
        self.memory_activity: Dict[str, deque] = defaultdict(lambda: deque(maxlen=100))
        
        # è®°å¿†å¤„ç†å‚æ•°
        self.attention_threshold = 0.7
        self.consolidation_threshold = 0.8
        self.forgetting_threshold = 0.1
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_memories': 0,
            'consolidated_memories': 0,
            'concepts_formed': 0,
            'knowledge_distilled': 0,
            'successful_retrievals': 0,
            'failed_retrievals': 0
        }
        
        # å¹¶å‘å¤„ç†
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.lock = threading.RLock()
        
        logger.info(f"æµ·é©¬ä½“è®°å¿†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ - å®¹é‡: {max_memory_size}, ç»´åº¦: {embedding_dim}")
    
    # ==================== æ¦‚å¿µå½¢æˆå’ŒæŠ½è±¡åŒ–æœºåˆ¶ ====================
    
    def form_concepts_from_memories(self, memory_ids: List[str]) -> List[str]:
        """ä»è®°å¿†ä¸­å½¢æˆæ¦‚å¿µ"""
        if len(memory_ids) < 3:
            return []
        
        with self.lock:
            try:
                # æ”¶é›†è®°å¿†æ•°æ®
                memories = [self.memories[mid] for mid in memory_ids if mid in self.memories]
                if len(memories) < 3:
                    return []
                
                # å‡†å¤‡åµŒå…¥å‘é‡
                embeddings = []
                for memory in memories:
                    if memory.vector_embedding is not None:
                        embeddings.append(torch.tensor(memory.vector_embedding, dtype=torch.float32))
                    else:
                        # ç”ŸæˆéšæœºåµŒå…¥å‘é‡
                        embeddings.append(torch.randn(self.embedding_dim, dtype=torch.float32))
                
                if not embeddings:
                    return []
                
                # æ¦‚å¿µç½‘ç»œå¤„ç†
                embedding_tensor = torch.stack(embeddings)
                concept_result = self.concept_network(embedding_tensor)
                
                # åˆ›å»ºæ¦‚å¿µ
                concept_ids = []
                for i, memory in enumerate(memories):
                    if i >= len(concept_result['abstraction_probs']):
                        break
                    
                    # ç¡®å®šæŠ½è±¡çº§åˆ«
                    abstraction_probs = concept_result['abstraction_probs'][i]
                    abstraction_level = torch.argmax(abstraction_probs).item()
                    
                    # ç¡®å®šæ¦‚å¿µç±»å‹
                    type_probs = concept_result['type_probs'][i]
                    concept_type = torch.argmax(type_probs).item()
                    
                    # åˆ›å»ºæ¦‚å¿µ
                    concept_id = str(uuid.uuid4())
                    prototype = concept_result['encoded_concepts'][i].detach().float().numpy()
                    
                    concept = Concept(
                        concept_id=concept_id,
                        name=f"æ¦‚å¿µ_{concept_type}_{len(self.concepts)}",
                        definition=f"åŸºäº{len(memories)}ä¸ªè®°å¿†å½¢æˆçš„æŠ½è±¡æ¦‚å¿µ",
                        attributes=self._extract_concept_attributes(memory),
                        examples=[memory.content for memory in memories[:5]],
                        abstraction_level=abstraction_level,
                        prototype_embedding=prototype,
                        constituent_memories=[m.memory_id for m in memories],
                        related_concepts=set(),
                        formation_time=time.time(),
                        confidence_score=float(torch.max(abstraction_probs))
                    )
                    
                    self.concepts[concept_id] = concept
                    self.semantic_network.add_concept(concept)
                    
                    # å»ºç«‹è®°å¿†ä¸æ¦‚å¿µçš„å…³è”
                    memory.concept_level = abstraction_level
                    memory.associations.add(concept_id)
                    
                    concept_ids.append(concept_id)
                
                self.stats['concepts_formed'] += len(concept_ids)
                logger.info(f"å½¢æˆ{len(concept_ids)}ä¸ªæ–°æ¦‚å¿µ")
                
                return concept_ids
                
            except Exception as e:
                logger.error(f"æ¦‚å¿µå½¢æˆå¤±è´¥: {str(e)}")
                return []
    
    def _extract_concept_attributes(self, memory: Memory) -> Set[str]:
        """æå–æ¦‚å¿µå±æ€§"""
        attributes = set()
        
        # ä»å†…å®¹æå–å±æ€§
        if isinstance(memory.content, str):
            content_lower = memory.content.lower()
            
            # å…³é”®è¯æå–
            keywords = ['å­¦ä¹ ', 'å·¥ä½œ', 'ç”Ÿæ´»', 'æœ‹å‹', 'å®¶åº­', 'æ—¶é—´', 'åœ°ç‚¹', 'å»ºé€ ', 'åˆ›é€ ']
            for keyword in keywords:
                if keyword in content_lower:
                    attributes.add(keyword)
            
            # æƒ…æ„Ÿå±æ€§
            positive_words = ['å¥½', 'æ£’', 'å–œæ¬¢', 'å¿«ä¹', 'æˆåŠŸ', 'æ»¡æ„']
            negative_words = ['å', 'å·®', 'è®¨åŒ', 'ç—›è‹¦', 'å¤±è´¥', 'ä¸æ»¡']
            
            for word in positive_words:
                if word in content_lower:
                    attributes.add('positive')
                    break
            
            for word in negative_words:
                if word in content_lower:
                    attributes.add('negative')
                    break
        
        # æ·»åŠ è®°å¿†ç±»å‹å±æ€§
        attributes.add(memory.memory_type)
        
        # æ·»åŠ å¥–åŠ±å±æ€§
        if memory.reward_value > 0.5:
            attributes.add('high_reward')
        elif memory.reward_value < -0.5:
            attributes.add('low_reward')
        
        return attributes
    
    # ==================== çŸ¥è¯†è’¸é¦å’Œå‹ç¼©å­˜å‚¨ ====================
    
    def distill_knowledge(self, memory_ids: List[str]) -> Optional[str]:
        """çŸ¥è¯†è’¸é¦ - å‹ç¼©å­˜å‚¨è®°å¿†"""
        if len(memory_ids) < 5:
            return None
        
        with self.lock:
            try:
                # æ”¶é›†è®°å¿†
                memories = [self.memories[mid] for mid in memory_ids if mid in self.memories]
                if len(memories) < 5:
                    return None
                
                # å‡†å¤‡åµŒå…¥å‘é‡
                embeddings = []
                valid_memories = []
                
                for memory in memories:
                    if memory.vector_embedding is not None:
                        embeddings.append(torch.tensor(memory.vector_embedding, dtype=torch.float32))
                        valid_memories.append(memory)
                
                if len(embeddings) < 5:
                    return None
                
                # æ‰§è¡ŒçŸ¥è¯†è’¸é¦
                embedding_tensor = torch.stack(embeddings)
                distillation_result = self.knowledge_distiller(embedding_tensor)
                
                # åˆ›å»ºè’¸é¦çŸ¥è¯†å¯¹è±¡
                knowledge_id = str(uuid.uuid4())
                compressed_embedding = distillation_result['compressed_embedding'].detach().float().numpy()
                
                # æå–å…³é”®ç‰¹å¾
                key_features = self._extract_key_features(valid_memories)
                
                # è®¡ç®—å‹ç¼©æ¯”ä¾‹
                original_size = len(embeddings) * self.embedding_dim
                compressed_size = compressed_embedding.size
                compression_ratio = original_size / max(compressed_size, 1)
                
                # è®¡ç®—è´¨é‡åˆ†æ•°
                fidelity_score = float(distillation_result['fidelity_score'])
                generalization_score = float(distillation_result['generalization_score'])
                quality_score = (fidelity_score + generalization_score) / 2
                
                distilled_knowledge = DistilledKnowledge(
                    knowledge_id=knowledge_id,
                    original_memory_ids=[m.memory_id for m in valid_memories],
                    compressed_embedding=compressed_embedding,
                    key_features=key_features,
                    compression_ratio=compression_ratio,
                    fidelity_score=fidelity_score,
                    formation_time=time.time(),
                    quality_score=quality_score
                )
                
                self.distilled_knowledge[knowledge_id] = distilled_knowledge
                self.stats['knowledge_distilled'] += 1
                
                # æ ‡è®°åŸå§‹è®°å¿†ä¸ºå·²è’¸é¦
                for memory in valid_memories:
                    memory.consolidation_level += 1
                
                logger.info(f"è’¸é¦çŸ¥è¯† {knowledge_id[:8]}... - å‹ç¼©æ¯”: {compression_ratio:.2f}, è´¨é‡: {quality_score:.3f}")
                
                return knowledge_id
                
            except Exception as e:
                logger.error(f"çŸ¥è¯†è’¸é¦å¤±è´¥: {str(e)}")
                return None
    
    def _extract_key_features(self, memories: List[Memory]) -> Dict[str, float]:
        """æå–å…³é”®ç‰¹å¾"""
        features = {}
        
        # ç»Ÿè®¡ç‰¹å¾
        rewards = [m.reward_value for m in memories]
        features['avg_reward'] = np.mean(rewards)
        features['reward_variance'] = np.var(rewards)
        features['max_reward'] = np.max(rewards)
        features['min_reward'] = np.min(rewards)
        
        # æƒ…æ„Ÿç‰¹å¾
        emotional_vals = [m.emotional_valence for m in memories]
        features['avg_emotional_valence'] = np.mean(emotional_vals)
        features['emotional_variance'] = np.var(emotional_vals)
        
        # è®°å¿†ç±»å‹åˆ†å¸ƒ
        type_counts = defaultdict(int)
        for memory in memories:
            type_counts[memory.memory_type] += 1
        
        for mem_type, count in type_counts.items():
            features[f'type_{mem_type}_ratio'] = count / len(memories)
        
        # åˆ›é€ åŠ›æ¯”ä¾‹
        creative_count = sum(1 for m in memories if m.creativity_flag)
        features['creativity_ratio'] = creative_count / len(memories)
        
        # æ¦‚å¿µå±‚çº§åˆ†å¸ƒ
        concept_levels = [m.concept_level for m in memories]
        features['avg_concept_level'] = np.mean(concept_levels)
        
        return features
    
    # ==================== è¯­ä¹‰è®°å¿†ç½‘ç»œ ====================
    
    def build_semantic_network(self):
        """æ„å»ºè¯­ä¹‰è®°å¿†ç½‘ç»œ"""
        with self.lock:
            # æ·»åŠ æ‰€æœ‰æ¦‚å¿µåˆ°è¯­ä¹‰ç½‘ç»œ
            for concept in self.concepts.values():
                self.semantic_network.add_concept(concept)
            
            # è®¡ç®—æ¦‚å¿µé—´å…³è”
            concept_ids = list(self.concepts.keys())
            for i, concept1_id in enumerate(concept_ids):
                for concept2_id in concept_ids[i+1:]:
                    similarity = self.semantic_network.compute_concept_similarity(
                        concept1_id, concept2_id
                    )
                    
                    if similarity > 0.5:
                        self.semantic_network.add_association(
                            concept1_id, concept2_id, similarity
                        )
                        
                        # æ›´æ–°æ¦‚å¿µçš„å…³è”é›†åˆ
                        self.concepts[concept1_id].related_concepts.add(concept2_id)
                        self.concepts[concept2_id].related_concepts.add(concept1_id)
            
            logger.info(f"è¯­ä¹‰ç½‘ç»œæ„å»ºå®Œæˆ - {len(self.concepts)}ä¸ªæ¦‚å¿µ, "
                       f"{sum(len(connections) for connections in self.semantic_network.edges.values()) // 2}ä¸ªå…³è”")
    
    def find_semantic_relationships(self, concept_id: str) -> List[Dict[str, Any]]:
        """æŸ¥æ‰¾æ¦‚å¿µçš„è¯­ä¹‰å…³ç³»"""
        if concept_id not in self.semantic_network.nodes:
            return []
        
        relationships = []
        
        # ç›´æ¥å…³è”
        direct_relations = self.semantic_network.get_related_concepts(concept_id, threshold=0.3)
        for related_id, strength in direct_relations:
            relationships.append({
                'type': 'direct',
                'target': related_id,
                'target_name': self.concepts[related_id].name,
                'strength': strength,
                'description': f"ç›´æ¥å…³è” - å¼ºåº¦: {strength:.3f}"
            })
        
        # è¯­ä¹‰ç›¸ä¼¼
        for other_id in self.concepts:
            if other_id != concept_id:
                similarity = self.semantic_network.compute_concept_similarity(concept_id, other_id)
                if similarity > 0.6:
                    relationships.append({
                        'type': 'similarity',
                        'target': other_id,
                        'target_name': self.concepts[other_id].name,
                        'strength': similarity,
                        'description': f"è¯­ä¹‰ç›¸ä¼¼ - ç›¸ä¼¼åº¦: {similarity:.3f}"
                    })
        
        # æŒ‰å¼ºåº¦æ’åº
        relationships.sort(key=lambda x: x['strength'], reverse=True)
        return relationships[:10]  # è¿”å›å‰10ä¸ªå…³ç³»
    
    # ==================== è®°å¿†æå–å’Œå…³è” ====================
    
    def retrieve_memories(self, 
                         query: Any, 
                         top_k: int = 10,
                         similarity_threshold: float = 0.6) -> List[Dict[str, Any]]:
        """è®°å¿†æå–å’Œå…³è”æ£€ç´¢"""
        start_time = time.time()
        
        with self.lock:
            try:
                # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
                query_embedding = self._create_embedding(query)
                
                # è®¡ç®—ä¸æ‰€æœ‰è®°å¿†çš„ç›¸ä¼¼åº¦
                similarities = []
                
                for memory_id, memory in self.memories.items():
                    similarity = self._compute_similarity(query_embedding, memory)
                    
                    if similarity >= similarity_threshold:
                        similarities.append((memory, similarity))
                
                # æŒ‰ç›¸ä¼¼åº¦æ’åº
                similarities.sort(key=lambda x: x[1], reverse=True)
                
                # è·å–top_kç»“æœ
                results = []
                for memory, similarity in similarities[:top_k]:
                    # è·å–å…³è”è®°å¿†
                    related_memories = self._find_associated_memories(memory)
                    
                    # è·å–ç›¸å…³æ¦‚å¿µ
                    related_concepts = self._find_related_concepts(memory)
                    
                    result = {
                        'memory': memory,
                        'similarity_score': similarity,
                        'related_memories': related_memories,
                        'related_concepts': related_concepts,
                        'association_strength': self._calculate_association_strength(memory, query_embedding)
                    }
                    
                    results.append(result)
                
                # æ›´æ–°æ£€ç´¢ç»Ÿè®¡
                retrieval_time = time.time() - start_time
                if results:
                    self.stats['successful_retrievals'] += 1
                else:
                    self.stats['failed_retrievals'] += 1
                
                logger.info(f"è®°å¿†æ£€ç´¢å®Œæˆ - æ‰¾åˆ°{len(results)}ä¸ªç»“æœ, è€—æ—¶: {retrieval_time*1000:.1f}ms")
                
                return results
                
            except Exception as e:
                logger.error(f"è®°å¿†æ£€ç´¢å¤±è´¥: {str(e)}")
                self.stats['failed_retrievals'] += 1
                return []
    
    def _create_embedding(self, content: Any) -> np.ndarray:
        """åˆ›å»ºå‘é‡åµŒå…¥"""
        if isinstance(content, np.ndarray):
            return content
        
        if isinstance(content, str):
            # ç®€å•çš„æ–‡æœ¬åµŒå…¥ (å®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨æ›´å¥½çš„æ–¹æ³•)
            hash_value = hash(content)
            np.random.seed(hash_value % (2**32))
            return np.random.randn(self.embedding_dim).astype(np.float32)
        
        # å…¶ä»–ç±»å‹çš„å¤„ç†
        content_str = str(content)
        hash_value = hash(content_str)
        np.random.seed(hash_value % (2**32))
        return np.random.randn(self.embedding_dim).astype(np.float32)
    
    def _compute_similarity(self, query_embedding: np.ndarray, memory: Memory) -> float:
        """è®¡ç®—ç›¸ä¼¼åº¦"""
        if memory.vector_embedding is None:
            # ä¸ºè®°å¿†ç”Ÿæˆå‘é‡è¡¨ç¤º
            memory.vector_embedding = self._create_embedding(memory.content)
        
        # ä½™å¼¦ç›¸ä¼¼åº¦
        query_norm = np.linalg.norm(query_embedding)
        memory_norm = np.linalg.norm(memory.vector_embedding)
        
        if query_norm == 0 or memory_norm == 0:
            return 0.0
        
        similarity = np.dot(query_embedding, memory.vector_embedding) / (query_norm * memory_norm)
        
        # åº”ç”¨è®°å¿†å¼ºåº¦æƒé‡
        weighted_similarity = similarity * (0.5 + memory.strength * 0.5)
        
        return max(0.0, weighted_similarity)
    
    def _find_associated_memories(self, memory: Memory) -> List[Memory]:
        """æŸ¥æ‰¾å…³è”è®°å¿†"""
        associated = []
        
        for assoc_id in memory.associations:
            if assoc_id in self.memories:
                associated.append(self.memories[assoc_id])
        
        # é€šè¿‡è¯­ä¹‰ç½‘ç»œæŸ¥æ‰¾ç›¸å…³æ¦‚å¿µçš„è®°å¿†
        for concept_id in memory.associations:
            if concept_id in self.concepts:
                concept = self.concepts[concept_id]
                for mem_id in concept.constituent_memories:
                    if mem_id in self.memories and mem_id != memory.memory_id:
                        associated.append(self.memories[mem_id])
        
        # å»é‡
        unique_associated = []
        seen_ids = set()
        for assoc_memory in associated:
            if assoc_memory.memory_id not in seen_ids:
                seen_ids.add(assoc_memory.memory_id)
                unique_associated.append(assoc_memory)
        
        return unique_associated[:5]  # è¿”å›å‰5ä¸ªå…³è”è®°å¿†
    
    def _find_related_concepts(self, memory: Memory) -> List[Concept]:
        """æŸ¥æ‰¾ç›¸å…³æ¦‚å¿µ"""
        related_concepts = []
        
        for concept_id in memory.associations:
            if concept_id in self.concepts:
                related_concepts.append(self.concepts[concept_id])
        
        # é€šè¿‡è¯­ä¹‰ç½‘ç»œæŸ¥æ‰¾ç›¸ä¼¼æ¦‚å¿µ
        for concept_id in memory.associations:
            if concept_id in self.semantic_network.edges:
                for related_id in self.semantic_network.edges[concept_id]:
                    if related_id in self.concepts and related_id not in [c.concept_id for c in related_concepts]:
                        related_concepts.append(self.concepts[related_id])
        
        return related_concepts[:3]  # è¿”å›å‰3ä¸ªç›¸å…³æ¦‚å¿µ
    
    def _calculate_association_strength(self, memory: Memory, query_embedding: np.ndarray) -> float:
        """è®¡ç®—å…³è”å¼ºåº¦"""
        # åŸºäºè®°å¿†å¼ºåº¦çš„å…³è”å¼ºåº¦
        base_strength = memory.strength
        
        # åŸºäºè®¿é—®é¢‘ç‡çš„å¢å¼º
        access_boost = 1.0 + math.log(memory.access_count + 1) * 0.1
        
        # åŸºäºæƒ…æ„Ÿææ€§çš„å¢å¼º
        emotion_boost = 1.0 + abs(memory.emotional_valence) * 0.2
        
        # åŸºäºåˆ›é€ åŠ›çš„å¢å¼º
        creativity_boost = 1.1 if memory.creativity_flag else 1.0
        
        total_strength = base_strength * access_boost * emotion_boost * creativity_boost
        return min(1.0, total_strength)
    
    # ==================== é•¿æœŸè®°å¿†å·©å›º ====================
    
    def consolidate_memories(self, force: bool = False) -> Dict[str, Any]:
        """é•¿æœŸè®°å¿†å·©å›º"""
        start_time = time.time()
        
        with self.lock:
            try:
                current_time = time.time()
                
                # æ£€æŸ¥å·©å›ºæ¡ä»¶
                if not force and not self._should_consolidate():
                    return {
                        'status': 'skipped',
                        'reason': 'æœªè¾¾åˆ°å·©å›ºæ¡ä»¶',
                        'consolidation_time': None
                    }
                
                logger.info("å¼€å§‹è®°å¿†å·©å›ºè¿‡ç¨‹...")
                
                # 1. é‡æ–°è®¡ç®—è®°å¿†å¼ºåº¦
                self._recalculate_memory_strengths()
                
                # 2. åº”ç”¨é—å¿˜æœºåˆ¶
                forgotten_count = self._apply_forgetting_mechanism()
                
                # 3. æ¦‚å¿µå½¢æˆ
                new_concepts = self._trigger_concept_formation()
                
                # 4. çŸ¥è¯†è’¸é¦
                new_distilled_knowledge = self._trigger_knowledge_distillation()
                
                # 5. è¯­ä¹‰ç½‘ç»œæ›´æ–°
                self.build_semantic_network()
                
                # 6. è®°å¿†å·©å›º
                consolidated_count = 0
                for memory in list(self.memories.values()):
                    if (memory.strength > self.consolidation_threshold and 
                        memory.consolidation_level < 5):
                        memory.consolidation_level += 1
                        consolidated_count += 1
                
                # æ›´æ–°ç»Ÿè®¡
                self.stats['consolidated_memories'] += consolidated_count
                
                consolidation_time = time.time() - start_time
                
                result = {
                    'status': 'success',
                    'consolidation_time': current_time,
                    'processing_time': consolidation_time,
                    'consolidated_memories': consolidated_count,
                    'forgotten_memories': forgotten_count,
                    'new_concepts': len(new_concepts),
                    'new_distilled_knowledge': len(new_distilled_knowledge),
                    'total_memories': len(self.memories),
                    'total_concepts': len(self.concepts),
                    'total_distilled_knowledge': len(self.distilled_knowledge)
                }
                
                logger.info(f"è®°å¿†å·©å›ºå®Œæˆ - å·©å›º{consolidated_count}ä¸ªè®°å¿†, "
                           f"é—å¿˜{forgotten_count}ä¸ªè®°å¿†, è€—æ—¶{consolidation_time:.2f}ç§’")
                
                return result
                
            except Exception as e:
                logger.error(f"è®°å¿†å·©å›ºå¤±è´¥: {str(e)}")
                return {'status': 'error', 'error': str(e)}
    
    def _should_consolidate(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿›è¡Œå·©å›º"""
        # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾å·©å›ºæ—¶é—´
        current_hour = int(time.time() / 3600) % 24
        if current_hour == self.consolidation_hour:
            return True
        
        # æˆ–è€…è®°å¿†æ•°é‡è¾¾åˆ°é˜ˆå€¼
        if len(self.memories) > self.max_memory_size * 0.8:
            return True
        
        # æˆ–è€…æœ‰è¶³å¤Ÿçš„å¼±è®°å¿†éœ€è¦å·©å›º
        weak_memories = sum(1 for m in self.memories.values() if m.strength < 0.3)
        if weak_memories > len(self.memories) * 0.3:
            return True
        
        return False
    
    def _recalculate_memory_strengths(self):
        """é‡æ–°è®¡ç®—è®°å¿†å¼ºåº¦"""
        current_time = time.time()
        
        for memory in self.memories.values():
            # æ—¶é—´è¡°å‡
            hours_elapsed = (current_time - memory.timestamp) / 3600
            time_decay = math.exp(-0.1 * hours_elapsed)
            
            # è®¿é—®é¢‘ç‡å¢å¼º
            access_boost = 1.0 + math.log(memory.access_count + 1) * 0.1
            
            # æƒ…æ„Ÿæƒé‡
            emotional_weight = 1.0 + abs(memory.emotional_valence) * 0.3
            
            # å¥–åŠ±æƒé‡
            reward_weight = 1.0 + max(0, memory.reward_value) * 0.5
            
            # åˆ›é€ åŠ›æƒé‡
            creativity_weight = 1.1 if memory.creativity_flag else 1.0
            
            # è®°å¿†ç±»å‹æƒé‡
            type_weights = {
                'episodic': 1.0,
                'semantic': 1.2,
                'procedural': 1.1,
                'creative': 1.3
            }
            type_weight = type_weights.get(memory.memory_type, 1.0)
            
            # ç»¼åˆå¼ºåº¦è®¡ç®—
            memory.strength = (time_decay * access_boost * emotional_weight * 
                             reward_weight * creativity_weight * type_weight)
            
            # ç¡®ä¿å¼ºåº¦åœ¨åˆç†èŒƒå›´å†…
            memory.strength = max(0.001, min(1.0, memory.strength))
    
    def _apply_forgetting_mechanism(self) -> int:
        """åº”ç”¨é—å¿˜æœºåˆ¶"""
        forgotten_count = 0
        memories_to_remove = []
        
        for memory_id, memory in self.memories.items():
            # é—å¿˜æ¡ä»¶
            should_forget = (
                memory.reward_value < self.forgetting_threshold or
                memory.strength < 0.005 or
                (memory.access_count == 0 and memory.strength < 0.01)
            )
            
            if should_forget:
                memories_to_remove.append(memory_id)
        
        # ç§»é™¤é—å¿˜çš„è®°å¿†
        for memory_id in memories_to_remove:
            del self.memories[memory_id]
            if memory_id in self.memory_activity:
                del self.memory_activity[memory_id]
            forgotten_count += 1
        
        return forgotten_count
    
    def _trigger_concept_formation(self) -> List[str]:
        """è§¦å‘æ¦‚å¿µå½¢æˆ"""
        # æŒ‰ç›¸ä¼¼åº¦åˆ†ç»„è®°å¿†
        memory_groups = self._group_memories_by_similarity()
        
        new_concepts = []
        for group in memory_groups:
            if len(group) >= 3:
                concept_ids = self.form_concepts_from_memories(group)
                new_concepts.extend(concept_ids)
        
        return new_concepts
    
    def _trigger_knowledge_distillation(self) -> List[str]:
        """è§¦å‘çŸ¥è¯†è’¸é¦"""
        # æŒ‰ç±»å‹åˆ†ç»„è®°å¿†
        memory_groups = self._group_memories_by_type()
        
        new_distilled_knowledge = []
        for group in memory_groups:
            if len(group) >= 5:
                knowledge_id = self.distill_knowledge(group)
                if knowledge_id:
                    new_distilled_knowledge.append(knowledge_id)
        
        return new_distilled_knowledge
    
    def _group_memories_by_similarity(self, threshold: float = 0.7) -> List[List[str]]:
        """æŒ‰ç›¸ä¼¼åº¦åˆ†ç»„è®°å¿†"""
        memory_ids = list(self.memories.keys())
        groups = []
        used_memories = set()
        
        for i, memory_id in enumerate(memory_ids):
            if memory_id in used_memories:
                continue
            
            group = [memory_id]
            used_memories.add(memory_id)
            
            for j, other_id in enumerate(memory_ids[i+1:], i+1):
                if other_id in used_memories:
                    continue
                
                similarity = self._compute_memory_similarity(memory_id, other_id)
                if similarity > threshold:
                    group.append(other_id)
                    used_memories.add(other_id)
            
            groups.append(group)
        
        return groups
    
    def _group_memories_by_type(self) -> List[List[str]]:
        """æŒ‰ç±»å‹åˆ†ç»„è®°å¿†"""
        type_groups = defaultdict(list)
        
        for memory_id, memory in self.memories.items():
            type_groups[memory.memory_type].append(memory_id)
        
        return [group for group in type_groups.values() if len(group) >= 5]
    
    def _compute_memory_similarity(self, memory_id1: str, memory_id2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªè®°å¿†çš„ç›¸ä¼¼åº¦"""
        if memory_id1 not in self.memories or memory_id2 not in self.memories:
            return 0.0
        
        memory1 = self.memories[memory_id1]
        memory2 = self.memories[memory_id2]
        
        # ç¡®ä¿è®°å¿†æœ‰å‘é‡è¡¨ç¤º
        if memory1.vector_embedding is None:
            memory1.vector_embedding = self._create_embedding(memory1.content).astype(np.float32)
        if memory2.vector_embedding is None:
            memory2.vector_embedding = self._create_embedding(memory2.content).astype(np.float32)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = np.dot(memory1.vector_embedding, memory2.vector_embedding) / (
            np.linalg.norm(memory1.vector_embedding) * np.linalg.norm(memory2.vector_embedding) + 1e-8
        )
        
        return max(0.0, similarity)
    
    # ==================== ä¸»è¦æ¥å£æ–¹æ³• ====================
    
    def store_memory(self, 
                    content: Any,
                    memory_type: str = "episodic",
                    reward_value: float = 0.0,
                    emotional_valence: float = 0.0,
                    creativity_flag: bool = False) -> str:
        """å­˜å‚¨æ–°è®°å¿†"""
        with self.lock:
            memory_id = str(uuid.uuid4())
            
            # åˆ›å»ºè®°å¿†
            memory = Memory(
                memory_id=memory_id,
                content=content,
                vector_embedding=self._create_embedding(content),
                concept_level=0,
                semantic_tags=self._extract_semantic_tags(content),
                associations=set(),
                timestamp=time.time(),
                strength=1.0,
                access_count=0,
                consolidation_level=0,
                memory_type=memory_type,
                reward_value=reward_value,
                emotional_valence=emotional_valence,
                creativity_flag=creativity_flag
            )
            
            self.memories[memory_id] = memory
            self.working_memory.append(memory_id)
            
            # å¯åŠ¨å¼‚æ­¥æ¦‚å¿µå½¢æˆ
            self.executor.submit(self._async_concept_detection, memory_id)
            
            self.stats['total_memories'] += 1
            logger.info(f"å­˜å‚¨è®°å¿†: {memory_id[:8]}... ({memory_type})")
            
            return memory_id
    
    def _extract_semantic_tags(self, content: Any) -> Set[str]:
        """æå–è¯­ä¹‰æ ‡ç­¾"""
        tags = set()
        
        if isinstance(content, str):
            content_lower = content.lower()
            
            # å…³é”®è¯æ ‡ç­¾
            keywords = ['æ˜¯', 'æœ‰', 'è¢«', 'è¿›è¡Œ', 'å®Œæˆ', 'å¼€å§‹', 'ç»“æŸ', 'å­¦ä¹ ', 
                       'å·¥ä½œ', 'ç”Ÿæ´»', 'æœ‹å‹', 'å®¶åº­', 'æ—¶é—´', 'åœ°ç‚¹', 'åŸå› ', 'ç»“æœ',
                       'å»ºé€ ', 'åˆ›é€ ', 'å‘ç°', 'ç†è§£', 'å®ç°', 'æ”¹è¿›', 'ä¼˜åŒ–']
            
            for keyword in keywords:
                if keyword in content_lower:
                    tags.add(keyword)
            
            # æƒ…æ„Ÿæ ‡ç­¾
            positive_words = ['å¥½', 'æ£’', 'å–œæ¬¢', 'å¿«ä¹', 'æˆåŠŸ', 'æ»¡æ„', 'å…´å¥‹']
            negative_words = ['å', 'å·®', 'è®¨åŒ', 'ç—›è‹¦', 'å¤±è´¥', 'ä¸æ»¡', 'å¤±æœ›']
            
            for word in positive_words:
                if word in content_lower:
                    tags.add('positive')
                    break
            
            for word in negative_words:
                if word in content_lower:
                    tags.add('negative')
                    break
            
            # æ—¶é—´æ ‡ç­¾
            time_indicators = ['ä»Šå¤©', 'æ˜¨å¤©', 'æ˜å¤©', 'ç°åœ¨', 'ä»¥å‰', 'ä»¥å', 'å°†æ¥', 'è¿‡å»']
            for indicator in time_indicators:
                if indicator in content_lower:
                    tags.add('temporal')
                    break
            
            # è¡ŒåŠ¨æ ‡ç­¾
            action_words = ['å»ºé€ ', 'åˆ›å»º', 'ä¿®å¤', 'æ”¹è¿›', 'åˆ†æ', 'è®¾è®¡', 'æµ‹è¯•', 'éªŒè¯']
            for action in action_words:
                if action in content_lower:
                    tags.add('action')
                    break
        
        return tags
    
    def _async_concept_detection(self, memory_id: str):
        """å¼‚æ­¥æ¦‚å¿µæ£€æµ‹"""
        try:
            # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©æ›´å¤šè®°å¿†ç§¯ç´¯
            time.sleep(1)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„ç›¸ä¼¼è®°å¿†
            memory = self.memories.get(memory_id)
            if not memory:
                return
            
            similar_memories = []
            for other_id, other_memory in self.memories.items():
                if other_id != memory_id:
                    similarity = self._compute_memory_similarity(memory_id, other_id)
                    if similarity > 0.6:
                        similar_memories.append(other_id)
            
            if len(similar_memories) >= 2:
                # è§¦å‘æ¦‚å¿µå½¢æˆ
                self.form_concepts_from_memories([memory_id] + similar_memories[:2])
                
        except Exception as e:
            logger.error(f"å¼‚æ­¥æ¦‚å¿µæ£€æµ‹å¤±è´¥: {str(e)}")
    
    def get_memory_statistics(self) -> Dict[str, Any]:
        """è·å–è®°å¿†ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            # è®°å¿†åˆ†å¸ƒç»Ÿè®¡
            memory_type_counts = defaultdict(int)
            consolidation_state_counts = defaultdict(int)
            concept_level_counts = defaultdict(int)
            
            for memory in self.memories.values():
                memory_type_counts[memory.memory_type] += 1
                consolidation_state_counts[memory.consolidation_level] += 1
                concept_level_counts[memory.concept_level] += 1
            
            # è®°å¿†å¼ºåº¦åˆ†å¸ƒ
            strength_distribution = {
                'weak': sum(1 for m in self.memories.values() if m.strength < 0.3),
                'medium': sum(1 for m in self.memories.values() if 0.3 <= m.strength < 0.7),
                'strong': sum(1 for m in self.memories.values() if m.strength >= 0.7)
            }
            
            # è¯­ä¹‰ç½‘ç»œç»Ÿè®¡
            semantic_stats = {
                'total_concepts': len(self.concepts),
                'total_associations': sum(len(connections) for connections in self.semantic_network.edges.values()) // 2,
                'avg_concept_connections': np.mean([len(connections) for connections in self.semantic_network.edges.values()]) if self.semantic_network.edges else 0
            }
            
            # æ£€ç´¢æ€§èƒ½
            total_retrievals = self.stats['successful_retrievals'] + self.stats['failed_retrievals']
            retrieval_accuracy = self.stats['successful_retrievals'] / max(total_retrievals, 1)
            
            return {
                'memory_overview': {
                    'total_memories': len(self.memories),
                    'memory_capacity_usage': len(self.memories) / self.max_memory_size,
                    'working_memory_size': len(self.working_memory)
                },
                'memory_distribution': {
                    'by_type': dict(memory_type_counts),
                    'by_consolidation_level': dict(consolidation_state_counts),
                    'by_concept_level': dict(concept_level_counts),
                    'by_strength': strength_distribution
                },
                'conceptual_stats': {
                    'total_concepts': len(self.concepts),
                    'concepts_formed': self.stats['concepts_formed'],
                    'semantic_network': semantic_stats
                },
                'knowledge_stats': {
                    'total_distilled_knowledge': len(self.distilled_knowledge),
                    'knowledge_distilled': self.stats['knowledge_distilled'],
                    'avg_compression_ratio': np.mean([dk.compression_ratio for dk in self.distilled_knowledge.values()]) if self.distilled_knowledge else 0,
                    'avg_quality_score': np.mean([dk.quality_score for dk in self.distilled_knowledge.values()]) if self.distilled_knowledge else 0
                },
                'performance_stats': {
                    'retrieval_accuracy': retrieval_accuracy,
                    'successful_retrievals': self.stats['successful_retrievals'],
                    'failed_retrievals': self.stats['failed_retrievals'],
                    'consolidation_cycles': self.stats['consolidated_memories']
                },
                'system_stats': self.stats.copy()
            }
    
    def export_memory_state(self, filepath: str):
        """å¯¼å‡ºè®°å¿†çŠ¶æ€"""
        with self.lock:
            export_data = {
                'memories': {
                    mid: {
                        'memory_id': m.memory_id,
                        'content': str(m.content) if not isinstance(m.content, (str, int, float)) else m.content,
                        'concept_level': m.concept_level,
                        'semantic_tags': list(m.semantic_tags),
                        'associations': list(m.associations),
                        'timestamp': m.timestamp,
                        'strength': m.strength,
                        'access_count': m.access_count,
                        'consolidation_level': m.consolidation_level,
                        'memory_type': m.memory_type,
                        'reward_value': m.reward_value,
                        'emotional_valence': m.emotional_valence,
                        'creativity_flag': m.creativity_flag,
                        'vector_embedding': m.vector_embedding.tolist() if m.vector_embedding is not None else None
                    }
                    for mid, m in self.memories.items()
                },
                'concepts': {
                    cid: {
                        'concept_id': c.concept_id,
                        'name': c.name,
                        'definition': c.definition,
                        'attributes': list(c.attributes),
                        'examples': c.examples,
                        'abstraction_level': c.abstraction_level,
                        'prototype_embedding': c.prototype_embedding.tolist() if c.prototype_embedding is not None else None,
                        'constituent_memories': c.constituent_memories,
                        'related_concepts': list(c.related_concepts),
                        'formation_time': c.formation_time,
                        'confidence_score': c.confidence_score
                    }
                    for cid, c in self.concepts.items()
                },
                'distilled_knowledge': {
                    kid: {
                        'knowledge_id': dk.knowledge_id,
                        'original_memory_ids': dk.original_memory_ids,
                        'compressed_embedding': dk.compressed_embedding.tolist(),
                        'key_features': dk.key_features,
                        'compression_ratio': dk.compression_ratio,
                        'fidelity_score': dk.fidelity_score,
                        'formation_time': dk.formation_time,
                        'quality_score': dk.quality_score
                    }
                    for kid, dk in self.distilled_knowledge.items()
                },
                'stats': self.stats,
                'export_time': time.time()
            }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"è®°å¿†çŠ¶æ€å·²å¯¼å‡ºåˆ°: {filepath}")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.executor:
            self.executor.shutdown(wait=True)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("=" * 80)
    print("ğŸ§  æµ·é©¬ä½“è®°å¿†ç³»ç»Ÿæµ‹è¯•")
    print("   æ¦‚å¿µå½¢æˆ + çŸ¥è¯†è’¸é¦ + è¯­ä¹‰ç½‘ç»œ + è®°å¿†æå– + é•¿æœŸå·©å›º")
    print("=" * 80)
    
    # åˆ›å»ºè®°å¿†ç³»ç»Ÿ
    memory_system = HippocampusMemorySystem(max_memory_size=1000, embedding_dim=128)
    
    # å­˜å‚¨æµ‹è¯•è®°å¿†
    test_memories = [
        ("å­¦ä¼šäº†æ–°çš„ç¼–ç¨‹æŠ€æœ¯", "semantic", 0.8, 0.5, False),
        ("å®Œæˆäº†ä¸€ä¸ªé‡è¦é¡¹ç›®", "episodic", 0.9, 0.7, True),
        ("å’Œæœ‹å‹ä»¬ä¸€èµ·åº¦è¿‡äº†æ„‰å¿«çš„æ—¶å…‰", "episodic", 0.6, 0.8, False),
        ("å‘ç°äº†è§£å†³é—®é¢˜çš„æ–°æ–¹æ³•", "creative", 0.7, 0.6, True),
        ("å·¥ä½œä¸­é‡åˆ°äº†å›°éš¾", "episodic", -0.2, -0.3, False),
        ("å­¦ä¼šäº†é«˜æ•ˆçš„ç¼–ç¨‹æŠ€å·§", "semantic", 0.8, 0.5, False),
        ("åˆ›é€ äº†ä¸€ä¸ªæœ‰è¶£çš„åº”ç”¨", "creative", 0.9, 0.8, True),
        ("å›¢é˜Ÿåˆä½œå–å¾—äº†æˆåŠŸ", "episodic", 0.8, 0.7, False)
    ]
    
    print("\n1. å­˜å‚¨æµ‹è¯•è®°å¿†...")
    memory_ids = []
    for i, (content, mem_type, reward, emotion, creativity) in enumerate(test_memories):
        memory_id = memory_system.store_memory(
            content=content,
            memory_type=mem_type,
            reward_value=reward,
            emotional_valence=emotion,
            creativity_flag=creativity
        )
        memory_ids.append(memory_id)
        print(f"   è®°å¿† {i+1}: {content[:30]}...")
    
    # ç­‰å¾…å¼‚æ­¥å¤„ç†
    time.sleep(2)
    
    # æ£€ç´¢æµ‹è¯•
    print("\n2. è®°å¿†æ£€ç´¢æµ‹è¯•...")
    query_results = memory_system.retrieve_memories("ç¼–ç¨‹å­¦ä¹ ", top_k=5)
    print(f"   æŸ¥è¯¢'ç¼–ç¨‹å­¦ä¹ 'æ‰¾åˆ° {len(query_results)} ä¸ªç»“æœ:")
    for i, result in enumerate(query_results):
        memory = result['memory']
        print(f"     {i+1}. {memory.content} (ç›¸ä¼¼åº¦: {result['similarity_score']:.3f})")
    
    # æ¦‚å¿µå½¢æˆæµ‹è¯•
    print("\n3. æ¦‚å¿µå½¢æˆæµ‹è¯•...")
    if len(memory_ids) >= 3:
        # é€‰æ‹©å‡ ä¸ªç›¸ä¼¼çš„è®°å¿†è¿›è¡Œæ¦‚å¿µå½¢æˆ
        similar_memory_ids = memory_ids[:3]
        concepts = memory_system.form_concepts_from_memories(similar_memory_ids)
        print(f"   å½¢æˆäº† {len(concepts)} ä¸ªæ¦‚å¿µ")
        for concept_id in concepts[:2]:  # æ˜¾ç¤ºå‰2ä¸ªæ¦‚å¿µ
            concept = memory_system.concepts[concept_id]
            print(f"     - {concept.name}: {concept.definition}")
    
    # çŸ¥è¯†è’¸é¦æµ‹è¯•
    print("\n4. çŸ¥è¯†è’¸é¦æµ‹è¯•...")
    if len(memory_ids) >= 5:
        # é€‰æ‹©å¤šä¸ªè®°å¿†è¿›è¡Œè’¸é¦
        distillation_ids = memory_ids[:5]
        knowledge_id = memory_system.distill_knowledge(distillation_ids)
        if knowledge_id:
            knowledge = memory_system.distilled_knowledge[knowledge_id]
            print(f"   è’¸é¦çŸ¥è¯† ID: {knowledge_id[:8]}...")
            print(f"     å‹ç¼©æ¯”: {knowledge.compression_ratio:.2f}")
            print(f"     è´¨é‡åˆ†æ•°: {knowledge.quality_score:.3f}")
        else:
            print("   çŸ¥è¯†è’¸é¦å¤±è´¥")
    
    # è¯­ä¹‰ç½‘ç»œæµ‹è¯•
    print("\n5. è¯­ä¹‰ç½‘ç»œæµ‹è¯•...")
    memory_system.build_semantic_network()
    if memory_system.concepts:
        # æ‰¾ä¸€ä¸ªæ¦‚å¿µæŸ¥çœ‹å…¶å…³ç³»
        first_concept_id = list(memory_system.concepts.keys())[0]
        relationships = memory_system.find_semantic_relationships(first_concept_id)
        concept = memory_system.concepts[first_concept_id]
        print(f"   æ¦‚å¿µ '{concept.name}' æœ‰ {len(relationships)} ä¸ªè¯­ä¹‰å…³ç³»")
        for rel in relationships[:2]:  # æ˜¾ç¤ºå‰2ä¸ªå…³ç³»
            print(f"     - {rel['description']}")
    
    # è®°å¿†å·©å›ºæµ‹è¯•
    print("\n6. è®°å¿†å·©å›ºæµ‹è¯•...")
    consolidation_result = memory_system.consolidate_memories(force=True)
    print(f"   å·©å›ºç»“æœ: {consolidation_result['status']}")
    print(f"   å·©å›ºè®°å¿†æ•°: {consolidation_result.get('consolidated_memories', 0)}")
    print(f"   é—å¿˜è®°å¿†æ•°: {consolidation_result.get('forgotten_memories', 0)}")
    print(f"   æ–°æ¦‚å¿µæ•°: {consolidation_result.get('new_concepts', 0)}")
    print(f"   æ–°è’¸é¦çŸ¥è¯†æ•°: {consolidation_result.get('new_distilled_knowledge', 0)}")
    
    # ç³»ç»Ÿç»Ÿè®¡
    print("\n7. ç³»ç»Ÿç»Ÿè®¡...")
    stats = memory_system.get_memory_statistics()
    print(f"   æ€»è®°å¿†æ•°: {stats['memory_overview']['total_memories']}")
    print(f"   è®°å¿†ä½¿ç”¨ç‡: {stats['memory_overview']['memory_capacity_usage']:.1%}")
    print(f"   æ¦‚å¿µæ•°: {stats['conceptual_stats']['total_concepts']}")
    print(f"   æ£€ç´¢å‡†ç¡®ç‡: {stats['performance_stats']['retrieval_accuracy']:.1%}")
    
    print("\n" + "=" * 80)
    print("âœ… æµ·é©¬ä½“è®°å¿†ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")
    print("   ç³»ç»Ÿå·²å…·å¤‡å®Œæ•´çš„è®°å¿†å¤„ç†èƒ½åŠ›")
    print("=" * 80)
    
    # æ¸…ç†èµ„æº
    memory_system.cleanup()